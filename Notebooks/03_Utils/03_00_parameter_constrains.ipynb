{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils.constraints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter constraints\n",
    "\n",
    "> In this notebook I will try to implement weight constraint that keeps a specific set of paremeters positive."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is basically traversing the weights `PyTree` and applying a function over the weights, which souldn't be specifically hard given we have `jax.tree_util.tree_map`. Then we can add that to a usual training loop to ensure that we can keep reaplying the same constraint during training to, for example, keep a specific set of weights above 0 at all times.\n",
    "\n",
    "> Actually, we ended up using `flax.traverse_util.path_aware_map` because it keeps the names of the parent nodes so that we can check if we're inside a specific layer or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 16:42:08.061274: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-06 16:42:08.111834: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-06 16:42:10.456447: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], device_type='GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from jax import numpy as jnp\n",
    "import flax\n",
    "from flax.core import freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Sequence, Union\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "from jax import lax, random, numpy as jnp\n",
    "import flax\n",
    "from flax.core import freeze, unfreeze, FrozenDict\n",
    "from flax import linen as nn\n",
    "from flax import struct\n",
    "from flax.training import train_state\n",
    "\n",
    "import optax\n",
    "\n",
    "from clu import metrics\n",
    "from ml_collections import ConfigDict\n",
    "\n",
    "from einops import reduce\n",
    "\n",
    "from iqadatasets.datasets import *\n",
    "from fxlayers.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "> We're going to employ `iqadatasets` to ease the loading of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_train = TID2008(\"/media/disk/databases/BBDD_video_image/Image_Quality/TID/TID2008/\")\n",
    "dst_val = TID2013(\"/media/disk/databases/BBDD_video_image/Image_Quality/TID/TID2013/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 16:42:16.513531: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([384, 512, 3]), TensorShape([384, 512, 3]), TensorShape([]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, img_dist, mos = next(iter(dst_train.dataset))\n",
    "img.shape, img_dist.shape, mos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 16:42:16.806541: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([384, 512, 3]), TensorShape([384, 512, 3]), TensorShape([]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, img_dist, mos = next(iter(dst_val.dataset))\n",
    "img.shape, img_dist.shape, mos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BATCH_SIZE: 32\n",
       "EPOCHS: 5\n",
       "LEARNING_RATE: 0.0003"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    \"BATCH_SIZE\": 32,\n",
    "    \"EPOCHS\": 5,\n",
    "    \"LEARNING_RATE\": 3e-4,\n",
    "}\n",
    "config = ConfigDict(config)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_train_rdy = dst_train.dataset.batch(config.BATCH_SIZE, drop_remainder=True)\n",
    "dst_val_rdy = dst_val.dataset.batch(config.BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model we're going to use\n",
    "\n",
    "> It's going to be a very simple model just for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDN(nn.Module):\n",
    "    \"\"\"Generalized Divisive Normalization.\"\"\"\n",
    "    kernel_size: Union[int, Sequence[int]]\n",
    "    strides: int = 1\n",
    "    padding: str = \"SAME\"\n",
    "    apply_independently: bool = False\n",
    "    # feature_group_count: int = 1\n",
    "    kernel_init: Callable = nn.initializers.lecun_normal()\n",
    "    bias_init: Callable = nn.initializers.ones_init()\n",
    "    alpha: float = 2.\n",
    "    epsilon: float = 1/2 # Exponential of the denominator\n",
    "    eps: float = 1e-6 # Numerical stability in the denominator\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "                 inputs,\n",
    "                 ):\n",
    "        denom = nn.Conv(features=inputs.shape[-1], # Same output channels as input\n",
    "                        kernel_size=self.kernel_size if isinstance(self.kernel_size, Sequence) else [self.kernel_size]*2, \n",
    "                        strides=self.strides, \n",
    "                        padding=self.padding,\n",
    "                        # feature_group_count=self.feature_group_count,\n",
    "                        feature_group_count=inputs.shape[-1] if self.apply_independently else 1,\n",
    "                        kernel_init=self.kernel_init, \n",
    "                        bias_init=nn.initializers.constant(-1))(inputs**self.alpha)\n",
    "        # denom = nn.relu(denom)\n",
    "        return inputs / (denom**self.epsilon + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptNet(nn.Module):\n",
    "    \"\"\"IQA model inspired by the visual system.\"\"\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "                 inputs,\n",
    "                 **kwargs,\n",
    "                 ):\n",
    "        outputs = nn.Conv(features=3, kernel_size=(5,5), strides=1, padding=\"SAME\")(inputs)\n",
    "        outputs = GDN(kernel_size=1, strides=1, padding=\"SAME\", apply_independently=True)(outputs)\n",
    "        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n",
    "        outputs = nn.Conv(features=6, kernel_size=(5,5), strides=1, padding=\"SAME\")(outputs)\n",
    "        outputs = GDN(kernel_size=1, strides=1, padding=\"SAME\", apply_independently=False)(outputs)\n",
    "        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n",
    "        # outputs = GaborLayer(features=128, kernel_size=21, strides=1, padding=\"SAME\", fs=21)(outputs, **kwargs)\n",
    "        outputs = nn.Conv(features=128, kernel_size=(5,5), strides=1, padding=\"SAME\")(outputs)\n",
    "        outputs = GDN(kernel_size=1, strides=1, padding=\"SAME\", apply_independently=False)(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the metrics with `clu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "    \"\"\"Collection of metrics to be tracked during training.\"\"\"\n",
    "    loss: metrics.Average.from_output(\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `TrainState` doesn't include metrics, but it's very easy to subclass it so that it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    metrics: Metrics\n",
    "    state: FrozenDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a function that initializes the `TrainState` from a module, a rng key and some optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(module, key, tx, input_shape):\n",
    "    \"\"\"Creates the initial `TrainState`.\"\"\"\n",
    "    variables = module.init(key, jnp.ones(input_shape))\n",
    "    state, params = variables.pop('params')\n",
    "    return TrainState.create(\n",
    "        apply_fn=module.apply,\n",
    "        params=params,\n",
    "        state=state,\n",
    "        tx=tx,\n",
    "        metrics=Metrics.empty()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the training step\n",
    "\n",
    "> We want to write a function that takes the `TrainState` and a batch of data can performs an optimization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_correlation(vec1, vec2):\n",
    "    vec1 = vec1.squeeze()\n",
    "    vec2 = vec2.squeeze()\n",
    "    vec1_mean = vec1.mean()\n",
    "    vec2_mean = vec2.mean()\n",
    "    num = vec1-vec1_mean\n",
    "    num *= vec2-vec2_mean\n",
    "    num = num.sum()\n",
    "    denom = jnp.sqrt(jnp.sum((vec1-vec1_mean)**2))\n",
    "    denom *= jnp.sqrt(jnp.sum((vec2-vec2_mean)**2))\n",
    "    return num/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    \"\"\"Train for a single step.\"\"\"\n",
    "    img, img_dist, mos = batch\n",
    "    def loss_fn(params):\n",
    "        ## Forward pass through the model\n",
    "        img_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img, mutable=list(state.state.keys()), train=True)\n",
    "        img_dist_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img_dist, mutable=list(state.state.keys()), train=True)\n",
    "\n",
    "        ## Calculate the distance\n",
    "        dist = ((img_pred - img_dist_pred)**2).sum(axis=(1,2,3))**(1/2)\n",
    "        \n",
    "        ## Calculate pearson correlation\n",
    "        return pearson_correlation(dist, mos), updated_state\n",
    "    \n",
    "    (loss, updated_state), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics_updates = state.metrics.single_from_model_output(loss=loss)\n",
    "    metrics = state.metrics.merge(metrics_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    state = state.replace(state=updated_state)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In their example, they don't calculate the metrics at the same time. I think it is kind of a waste because it means having to perform a new forward pass, but we'll follow as of now. Let's define a function to perform metric calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def compute_metrics(*, state, batch):\n",
    "    \"\"\"Obtaining the metrics for a given batch.\"\"\"\n",
    "    img, img_dist, mos = batch\n",
    "    def loss_fn(params):\n",
    "        ## Forward pass through the model\n",
    "        img_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img, mutable=list(state.state.keys()), train=True)\n",
    "        img_dist_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img_dist, mutable=list(state.state.keys()), train=True)\n",
    "\n",
    "        ## Calculate the distance\n",
    "        dist = ((img_pred - img_dist_pred)**2).sum(axis=(1,2,3))**(1/2)\n",
    "        \n",
    "        ## Calculate pearson correlation\n",
    "        return pearson_correlation(dist, mos)\n",
    "    \n",
    "    metrics_updates = state.metrics.single_from_model_output(loss=loss_fn(state.params))\n",
    "    metrics = state.metrics.merge(metrics_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll instantiate the `TrainState` to get our parameters `PyTree`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = create_train_state(PerceptNet(), random.PRNGKey(0), optax.adam(config.LEARNING_RATE), input_shape=(1,384,512,3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can apply `flax.traverse_util.path_aware_map()` together with `jnp.clip()` to ensure that there are no negative numbers in our GDN parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Conv_0', 'GDN_0', 'Conv_1', 'GDN_1', 'Conv_2', 'GDN_2'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clipped_params = flax.traverse_util.path_aware_map(lambda path, v: jnp.clip(v, 0) if \"GDN\" in \"_\".join(path) else v, state.params)\n",
    "clipped_params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FrozenDict({\n",
       "     kernel: Array([[[[-0.20927475, -0.8931865 , -0.01957742,  0.39708266,\n",
       "                0.57847786,  0.4533468 ],\n",
       "              [ 0.06100697, -0.28133187,  0.50638074, -0.0699495 ,\n",
       "               -0.6131419 , -0.25098443],\n",
       "              [-0.38723496, -0.37814805,  0.07733525,  0.68892664,\n",
       "               -0.2935091 ,  0.3425824 ],\n",
       "              [-0.15717277, -0.65036553,  0.5565868 , -0.20142964,\n",
       "               -0.10007945,  0.19361018],\n",
       "              [-0.29459316, -0.65032095, -0.30642185, -0.52035373,\n",
       "                0.00798638,  0.73876256],\n",
       "              [-0.11728425, -0.7334936 , -0.70490634, -0.28179142,\n",
       "               -0.2679925 ,  0.08794045]]]], dtype=float32),\n",
       "     bias: Array([-1., -1., -1., -1., -1., -1.], dtype=float32),\n",
       " }),\n",
       " {'kernel': Array([[[[0.        , 0.        , 0.        , 0.39708266, 0.57847786,\n",
       "            0.4533468 ],\n",
       "           [0.06100697, 0.        , 0.50638074, 0.        , 0.        ,\n",
       "            0.        ],\n",
       "           [0.        , 0.        , 0.07733525, 0.68892664, 0.        ,\n",
       "            0.3425824 ],\n",
       "           [0.        , 0.        , 0.5565868 , 0.        , 0.        ,\n",
       "            0.19361018],\n",
       "           [0.        , 0.        , 0.        , 0.        , 0.00798638,\n",
       "            0.73876256],\n",
       "           [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "            0.08794045]]]], dtype=float32),\n",
       "  'bias': Array([0., 0., 0., 0., 0., 0.], dtype=float32)})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.params[\"GDN_1\"][\"Conv_0\"], clipped_params[\"GDN_1\"][\"Conv_0\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, our procedure will clip values both in `kernel` and in `bias`. If we want to restrict ourselves to only the kernel, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Conv_0', 'GDN_0', 'Conv_1', 'GDN_1', 'Conv_2', 'GDN_2'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clipped_params_kernel = flax.traverse_util.path_aware_map(lambda path, v: jnp.clip(v, 0) if (\"GDN\" in \"_\".join(path)) and (\"kernel\" in path) else v, state.params)\n",
    "clipped_params_kernel.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FrozenDict({\n",
       "     kernel: Array([[[[-0.20927475, -0.8931865 , -0.01957742,  0.39708266,\n",
       "                0.57847786,  0.4533468 ],\n",
       "              [ 0.06100697, -0.28133187,  0.50638074, -0.0699495 ,\n",
       "               -0.6131419 , -0.25098443],\n",
       "              [-0.38723496, -0.37814805,  0.07733525,  0.68892664,\n",
       "               -0.2935091 ,  0.3425824 ],\n",
       "              [-0.15717277, -0.65036553,  0.5565868 , -0.20142964,\n",
       "               -0.10007945,  0.19361018],\n",
       "              [-0.29459316, -0.65032095, -0.30642185, -0.52035373,\n",
       "                0.00798638,  0.73876256],\n",
       "              [-0.11728425, -0.7334936 , -0.70490634, -0.28179142,\n",
       "               -0.2679925 ,  0.08794045]]]], dtype=float32),\n",
       "     bias: Array([-1., -1., -1., -1., -1., -1.], dtype=float32),\n",
       " }),\n",
       " {'kernel': Array([[[[0.        , 0.        , 0.        , 0.39708266, 0.57847786,\n",
       "            0.4533468 ],\n",
       "           [0.06100697, 0.        , 0.50638074, 0.        , 0.        ,\n",
       "            0.        ],\n",
       "           [0.        , 0.        , 0.07733525, 0.68892664, 0.        ,\n",
       "            0.3425824 ],\n",
       "           [0.        , 0.        , 0.5565868 , 0.        , 0.        ,\n",
       "            0.19361018],\n",
       "           [0.        , 0.        , 0.        , 0.        , 0.00798638,\n",
       "            0.73876256],\n",
       "           [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "            0.08794045]]]], dtype=float32),\n",
       "  'bias': Array([-1., -1., -1., -1., -1., -1.], dtype=float32)})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.params[\"GDN_1\"][\"Conv_0\"], clipped_params_kernel[\"GDN_1\"][\"Conv_0\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to apply this at each step of the training process, we can wrap everything inside a function and add a \"constraint step\" into our training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def clip_layer_kernel(params, # PyTree containing the parameters to clip.\n",
    "                      layer_name: str, # String indicating the name of the layer. It can be a generic like \"Conv\" or specific like \"Conv_0\". \n",
    "                      a_min=None, # Min value to clip to.\n",
    "                      a_max=None, # Max value to clip to.\n",
    "                      ): # Same PyTree as `params` but with the corresponding values clipped.\n",
    "    return freeze(flax.traverse_util.path_aware_map(lambda path, v: jnp.clip(v, a_min, a_max) if (layer_name in \"_\".join(path)) and (\"kernel\" in path) else v, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def clip_layer(params, # PyTree containing the parameters to clip.\n",
    "               layer_name: str, # String indicating the name of the layer. It can be a generic like \"Conv\" or specific like \"Conv_0\". \n",
    "               a_min=None, # Min value to clip to.\n",
    "               a_max=None, # Max value to clip to.\n",
    "               ): # Same PyTree as `params` but with the corresponding values clipped.\n",
    "    return freeze(flax.traverse_util.path_aware_map(lambda path, v: jnp.clip(v, a_min, a_max) if layer_name in \"_\".join(path) else v, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    \"\"\"Train for a single step.\"\"\"\n",
    "    img, img_dist, mos = batch\n",
    "    def loss_fn(params):\n",
    "        ## Forward pass through the model\n",
    "        img_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img, mutable=list(state.state.keys()), train=True)\n",
    "        img_dist_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img_dist, mutable=list(state.state.keys()), train=True)\n",
    "\n",
    "        ## Calculate the distance\n",
    "        dist = ((img_pred - img_dist_pred)**2).sum(axis=(1,2,3))**(1/2)\n",
    "        \n",
    "        ## Calculate pearson correlation\n",
    "        return pearson_correlation(dist, mos), updated_state\n",
    "    \n",
    "    (loss, updated_state), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics_updates = state.metrics.single_from_model_output(loss=loss)\n",
    "    metrics = state.metrics.merge(metrics_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    state = state.replace(state=updated_state)\n",
    "    return state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDN(nn.Module):\n",
    "    \"\"\"Generalized Divisive Normalization.\"\"\"\n",
    "    kernel_size: Union[int, Sequence[int]]\n",
    "    strides: int = 1\n",
    "    padding: str = \"SAME\"\n",
    "    apply_independently: bool = False\n",
    "    # feature_group_count: int = 1\n",
    "    kernel_init: Callable = nn.initializers.lecun_normal()\n",
    "    bias_init: Callable = nn.initializers.ones_init()\n",
    "    alpha: float = 2.\n",
    "    epsilon: float = 1/2 # Exponential of the denominator\n",
    "    eps: float = 1e-6 # Numerical stability in the denominator\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "                 inputs,\n",
    "                 ):\n",
    "        denom = nn.Conv(features=inputs.shape[-1], # Same output channels as input\n",
    "                        kernel_size=self.kernel_size if isinstance(self.kernel_size, Sequence) else [self.kernel_size]*2, \n",
    "                        strides=self.strides, \n",
    "                        padding=self.padding,\n",
    "                        # feature_group_count=self.feature_group_count,\n",
    "                        feature_group_count=inputs.shape[-1] if self.apply_independently else 1,\n",
    "                        kernel_init=self.kernel_init, \n",
    "                        bias_init=nn.initializers.ones)(inputs**self.alpha)\n",
    "        # denom = nn.relu(denom)\n",
    "        return inputs / (denom**self.epsilon + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = create_train_state(PerceptNet(), random.PRNGKey(0), optax.adam(config.LEARNING_RATE), input_shape=(1,384,512,3))\n",
    "state = state.replace(params=clip_layer(state.params, \"GDN\", a_min=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 -> [Train] Loss: -0.6268874406814575 [Val] Loss: -0.6873180866241455\n",
      "CPU times: user 14.6 s, sys: 6.73 s, total: 21.3 s\n",
      "Wall time: 3.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(config.EPOCHS):\n",
    "    ## Training\n",
    "    for batch in dst_train_rdy.as_numpy_iterator():\n",
    "        state = train_step(state, batch)\n",
    "        state = state.replace(params=clip_layer(state.params, \"GDN\", a_min=0))\n",
    "        # state = compute_metrics(state=state, batch=batch)\n",
    "        break\n",
    "\n",
    "    ## Log the metrics\n",
    "    for name, value in state.metrics.compute().items():\n",
    "        metrics_history[f\"train_{name}\"].append(value)\n",
    "    \n",
    "    ## Empty the metrics\n",
    "    state = state.replace(metrics=state.metrics.empty())\n",
    "\n",
    "    ## Evaluation\n",
    "    for batch in dst_val_rdy.as_numpy_iterator():\n",
    "        state = compute_metrics(state=state, batch=batch)\n",
    "        break\n",
    "    for name, value in state.metrics.compute().items():\n",
    "        metrics_history[f\"val_{name}\"].append(value)\n",
    "    state = state.replace(metrics=state.metrics.empty())\n",
    "    \n",
    "    print(f'Epoch {epoch} -> [Train] Loss: {metrics_history[\"train_loss\"][-1]} [Val] Loss: {metrics_history[\"val_loss\"][-1]}')\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

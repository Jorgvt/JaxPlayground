{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification in JAX using `TrainState` (and `WandB`!) ((and checkpointing!!))\n",
    "\n",
    "> Adding checkpointing to the *WandB* example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 12:53:59.255932: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-27 12:53:59.310993: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-27 12:54:01.377224: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], device_type='GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Callable, Sequence, Union\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "from jax import lax, random, numpy as jnp\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax import linen as nn\n",
    "from flax import struct\n",
    "from flax.training import train_state\n",
    "from flax.training import orbax_utils\n",
    "\n",
    "import optax\n",
    "import orbax.checkpoint\n",
    "\n",
    "from clu import metrics\n",
    "from ml_collections import ConfigDict\n",
    "\n",
    "from einops import reduce\n",
    "import wandb\n",
    "\n",
    "from iqadatasets.datasets import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "\n",
    "> We'll be using MNIST from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (60000,), (10000, 28, 28, 1), (10000,))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train = X_train[:,:,:,None]/255.0\n",
    "X_test = X_test[:,:,:,None]/255.0\n",
    "Y_train = Y_train.astype(np.int32)\n",
    "Y_test = Y_test.astype(np.int32)\n",
    "\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_train = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "dst_val = tf.data.Dataset.from_tensor_slices((X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BATCH_SIZE: 256\n",
       "EPOCHS: 50\n",
       "LEARNING_RATE: 0.0003"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    \"BATCH_SIZE\": 256,\n",
    "    \"EPOCHS\": 50,\n",
    "    \"LEARNING_RATE\": 3e-4,\n",
    "}\n",
    "config = ConfigDict(config)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjorgvt\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/disk/users/vitojor/JaxPlayground/Notebooks/00_Supervised/wandb/run-20230427_125410-u9x7wdmf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jorgvt/MNIST_JAX/runs/u9x7wdmf' target=\"_blank\">Single_Forward</a></strong> to <a href='https://wandb.ai/jorgvt/MNIST_JAX' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jorgvt/MNIST_JAX' target=\"_blank\">https://wandb.ai/jorgvt/MNIST_JAX</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jorgvt/MNIST_JAX/runs/u9x7wdmf' target=\"_blank\">https://wandb.ai/jorgvt/MNIST_JAX/runs/u9x7wdmf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BATCH_SIZE: 256\n",
       "EPOCHS: 50\n",
       "LEARNING_RATE: 0.0003"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"MNIST_JAX\",\n",
    "           name=\"Single_Forward\",\n",
    "           job_type=\"training\",\n",
    "           config=config,\n",
    "           mode=\"online\",\n",
    "           )\n",
    "config = config\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_train_rdy = dst_train.batch(config.BATCH_SIZE)\n",
    "dst_val_rdy = dst_val.batch(config.BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model we're going to use\n",
    "\n",
    "> It's going to be a very simple model just for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "                 inputs,\n",
    "                 **kwargs,\n",
    "                 ):\n",
    "        outputs = nn.Conv(features=32, kernel_size=(3,3))(inputs)\n",
    "        outputs = nn.relu(outputs)\n",
    "        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n",
    "        outputs = nn.Conv(features=64, kernel_size=(3,3))(outputs)\n",
    "        outputs = nn.relu(outputs)\n",
    "        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n",
    "        outputs = reduce(outputs, \"b h w c -> b c\", reduction=\"mean\")\n",
    "        outputs = nn.Dense(10)(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the metrics with `clu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "    \"\"\"Collection of metrics to be tracked during training.\"\"\"\n",
    "    accuracy: metrics.Accuracy\n",
    "    loss: metrics.Average.from_output(\"loss\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `TrainState` doesn't include metrics, but it's very easy to subclass it so that it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    metrics: Metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a function that initializes the `TrainState` from a module, a rng key and some optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(module, key, tx, input_shape):\n",
    "    \"\"\"Creates the initial `TrainState`.\"\"\"\n",
    "    params = module.init(key, jnp.ones(input_shape))[\"params\"]\n",
    "    return TrainState.create(\n",
    "        apply_fn=module.apply,\n",
    "        params=params,\n",
    "        tx=tx,\n",
    "        metrics=Metrics.empty()\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the training step\n",
    "\n",
    "> We want to write a function that takes the `TrainState` and a batch of data can performs an optimization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    \"\"\"Train for a single step.\"\"\"\n",
    "    inputs, labels = batch\n",
    "    def loss_fn(params):\n",
    "        pred = state.apply_fn({\"params\": params}, inputs)\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(logits=pred, labels=labels).mean()\n",
    "        return loss, pred\n",
    "    (loss, pred), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    def compute_metrics(state):\n",
    "        metric_updates = state.metrics.single_from_model_output(\n",
    "            logits=pred, labels=labels, loss=loss,\n",
    "        )\n",
    "        metrics = state.metrics.merge(metric_updates)\n",
    "        state = state.replace(metrics=metrics)\n",
    "        return state\n",
    "    state = compute_metrics(state)\n",
    "    return state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In their example, they don't calculate the metrics at the same time. I think it is kind of a waste because it means having to perform a new forward pass, but we'll follow as of now. Let's define a function to perform metric calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def compute_metrics(*, state, batch):\n",
    "    \"\"\"Obtaining the metrics for a given batch.\"\"\"\n",
    "    inputs, labels = batch\n",
    "    pred = state.apply_fn({\"params\": state.params}, inputs)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=pred, labels=labels).mean()\n",
    "    metric_updates = state.metrics.single_from_model_output(\n",
    "        logits=pred, labels=labels, loss=loss,\n",
    "    )\n",
    "    metrics = state.metrics.merge(metric_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = create_train_state(Model(), random.PRNGKey(0), optax.adam(config.LEARNING_RATE), input_shape=(1,28,28,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before actually training the model we're going to set up the checkpointer to be able to save our trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "save_args = orbax_utils.save_args_from_target(state)\n",
    "# orbax_checkpointer.save(\"test_save\", state, save_args=save_args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to use versioning and automatic bookkeeping we need to wrap `PyTreeCheckpointer` with `orbax.checkpoint.CheckpointManager`. This allows us to customize the saving even more if we need to. As saving a model is an I/O operation, we may benefit from doing it asyncronously. This is as easy as using `AsyncCheckpointer` instead of `PyTreeCheckpointer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"train_accuracy\": [],\n",
    "    \"val_accuracy\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 12:54:25.313858: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [60000]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-04-27 12:54:28.345723: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [10000]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 -> [Train] Loss: 2.1820199489593506 | Accuracy: 0.2582666575908661 [Val] Loss: 1.9363574981689453 | Accuracy: 0.42100003361701965\n",
      "Epoch 1 -> [Train] Loss: 1.67693293094635 | Accuracy: 0.4978833496570587 [Val] Loss: 1.4757040739059448 | Accuracy: 0.5752000212669373\n",
      "Epoch 2 -> [Train] Loss: 1.377483606338501 | Accuracy: 0.5930666923522949 [Val] Loss: 1.2632259130477905 | Accuracy: 0.6459000110626221\n",
      "Epoch 3 -> [Train] Loss: 1.2066980600357056 | Accuracy: 0.6537666916847229 [Val] Loss: 1.1153720617294312 | Accuracy: 0.6894000172615051\n",
      "Epoch 4 -> [Train] Loss: 1.081371545791626 | Accuracy: 0.6956666707992554 [Val] Loss: 1.0010062456130981 | Accuracy: 0.7227000594139099\n",
      "Epoch 5 -> [Train] Loss: 0.9829501509666443 | Accuracy: 0.7260167002677917 [Val] Loss: 0.9094106554985046 | Accuracy: 0.7510000467300415\n",
      "Epoch 6 -> [Train] Loss: 0.9034146666526794 | Accuracy: 0.7497000098228455 [Val] Loss: 0.8354026079177856 | Accuracy: 0.770300030708313\n",
      "Epoch 7 -> [Train] Loss: 0.8378530740737915 | Accuracy: 0.7681166529655457 [Val] Loss: 0.7736756801605225 | Accuracy: 0.7862000465393066\n",
      "Epoch 8 -> [Train] Loss: 0.782595157623291 | Accuracy: 0.7839333415031433 [Val] Loss: 0.721613347530365 | Accuracy: 0.801300048828125\n",
      "Epoch 9 -> [Train] Loss: 0.7353770136833191 | Accuracy: 0.7968166470527649 [Val] Loss: 0.6770400404930115 | Accuracy: 0.8156000375747681\n",
      "Epoch 10 -> [Train] Loss: 0.6942520141601562 | Accuracy: 0.8086666464805603 [Val] Loss: 0.6380061507225037 | Accuracy: 0.8267000317573547\n",
      "Epoch 11 -> [Train] Loss: 0.6578672528266907 | Accuracy: 0.8182333707809448 [Val] Loss: 0.6035601496696472 | Accuracy: 0.8357000350952148\n",
      "Epoch 12 -> [Train] Loss: 0.6253001689910889 | Accuracy: 0.8272666931152344 [Val] Loss: 0.5729242563247681 | Accuracy: 0.844700038433075\n",
      "Epoch 13 -> [Train] Loss: 0.5958832502365112 | Accuracy: 0.8356500267982483 [Val] Loss: 0.5454649329185486 | Accuracy: 0.8524000644683838\n",
      "Epoch 14 -> [Train] Loss: 0.569243848323822 | Accuracy: 0.8437333703041077 [Val] Loss: 0.5207400321960449 | Accuracy: 0.8609000444412231\n",
      "Epoch 15 -> [Train] Loss: 0.5449531674385071 | Accuracy: 0.8503666520118713 [Val] Loss: 0.49839162826538086 | Accuracy: 0.8663000464439392\n",
      "Epoch 16 -> [Train] Loss: 0.5226361155509949 | Accuracy: 0.8562333583831787 [Val] Loss: 0.4781108796596527 | Accuracy: 0.8708000183105469\n",
      "Epoch 17 -> [Train] Loss: 0.5021227598190308 | Accuracy: 0.8616333603858948 [Val] Loss: 0.45969006419181824 | Accuracy: 0.8740000128746033\n",
      "Epoch 18 -> [Train] Loss: 0.4832536578178406 | Accuracy: 0.8670333623886108 [Val] Loss: 0.4427900016307831 | Accuracy: 0.8785000443458557\n",
      "Epoch 19 -> [Train] Loss: 0.4658797085285187 | Accuracy: 0.8720333576202393 [Val] Loss: 0.42743808031082153 | Accuracy: 0.8810000419616699\n",
      "Epoch 20 -> [Train] Loss: 0.44983699917793274 | Accuracy: 0.8764833211898804 [Val] Loss: 0.4134153425693512 | Accuracy: 0.8843000531196594\n",
      "Epoch 21 -> [Train] Loss: 0.43500810861587524 | Accuracy: 0.8805000185966492 [Val] Loss: 0.4005475640296936 | Accuracy: 0.8878000378608704\n",
      "Epoch 22 -> [Train] Loss: 0.4212695360183716 | Accuracy: 0.8841333389282227 [Val] Loss: 0.38852593302726746 | Accuracy: 0.8904000520706177\n",
      "Epoch 23 -> [Train] Loss: 0.4085208475589752 | Accuracy: 0.887583315372467 [Val] Loss: 0.37755367159843445 | Accuracy: 0.8936000466346741\n",
      "Epoch 24 -> [Train] Loss: 0.3966846764087677 | Accuracy: 0.8907666802406311 [Val] Loss: 0.3673703372478485 | Accuracy: 0.8962000608444214\n",
      "Epoch 25 -> [Train] Loss: 0.38564103841781616 | Accuracy: 0.8933833241462708 [Val] Loss: 0.3579704165458679 | Accuracy: 0.8970000147819519\n",
      "Epoch 26 -> [Train] Loss: 0.3753395974636078 | Accuracy: 0.8961166739463806 [Val] Loss: 0.34921061992645264 | Accuracy: 0.8986000418663025\n",
      "Epoch 27 -> [Train] Loss: 0.3657287061214447 | Accuracy: 0.8985333442687988 [Val] Loss: 0.3410508334636688 | Accuracy: 0.9005000591278076\n",
      "Epoch 28 -> [Train] Loss: 0.35672497749328613 | Accuracy: 0.9010500311851501 [Val] Loss: 0.3333248198032379 | Accuracy: 0.9026000499725342\n",
      "Epoch 29 -> [Train] Loss: 0.34829211235046387 | Accuracy: 0.9030166864395142 [Val] Loss: 0.3262018859386444 | Accuracy: 0.9036000370979309\n",
      "Epoch 30 -> [Train] Loss: 0.34040912985801697 | Accuracy: 0.9047999978065491 [Val] Loss: 0.3194579780101776 | Accuracy: 0.9045000672340393\n",
      "Epoch 31 -> [Train] Loss: 0.33296310901641846 | Accuracy: 0.9061999917030334 [Val] Loss: 0.3132016062736511 | Accuracy: 0.9064000248908997\n",
      "Epoch 32 -> [Train] Loss: 0.32594195008277893 | Accuracy: 0.9077666997909546 [Val] Loss: 0.3072311282157898 | Accuracy: 0.9080000519752502\n",
      "Epoch 33 -> [Train] Loss: 0.319314181804657 | Accuracy: 0.9093000292778015 [Val] Loss: 0.30153241753578186 | Accuracy: 0.9094000458717346\n",
      "Epoch 34 -> [Train] Loss: 0.31306859850883484 | Accuracy: 0.9110000133514404 [Val] Loss: 0.2962201237678528 | Accuracy: 0.9110000729560852\n",
      "Epoch 35 -> [Train] Loss: 0.3071337640285492 | Accuracy: 0.9126666784286499 [Val] Loss: 0.29112088680267334 | Accuracy: 0.9123000502586365\n",
      "Epoch 36 -> [Train] Loss: 0.3015291392803192 | Accuracy: 0.9140499830245972 [Val] Loss: 0.28640487790107727 | Accuracy: 0.9139000177383423\n",
      "Epoch 37 -> [Train] Loss: 0.2961737811565399 | Accuracy: 0.9154000282287598 [Val] Loss: 0.28188082575798035 | Accuracy: 0.9153000712394714\n",
      "Epoch 38 -> [Train] Loss: 0.29110246896743774 | Accuracy: 0.9168500304222107 [Val] Loss: 0.2775653302669525 | Accuracy: 0.916100025177002\n",
      "Epoch 39 -> [Train] Loss: 0.2862601578235626 | Accuracy: 0.9182167053222656 [Val] Loss: 0.2733447253704071 | Accuracy: 0.917400062084198\n",
      "Epoch 40 -> [Train] Loss: 0.28162434697151184 | Accuracy: 0.9193000197410583 [Val] Loss: 0.2694474756717682 | Accuracy: 0.9187000393867493\n",
      "Epoch 41 -> [Train] Loss: 0.2772080898284912 | Accuracy: 0.9207000136375427 [Val] Loss: 0.2655843198299408 | Accuracy: 0.9201000332832336\n",
      "Epoch 42 -> [Train] Loss: 0.2729730010032654 | Accuracy: 0.9217666983604431 [Val] Loss: 0.261908620595932 | Accuracy: 0.9214000701904297\n",
      "Epoch 43 -> [Train] Loss: 0.2688678205013275 | Accuracy: 0.9227833151817322 [Val] Loss: 0.2584514319896698 | Accuracy: 0.9222000241279602\n",
      "Epoch 44 -> [Train] Loss: 0.2649327516555786 | Accuracy: 0.92371666431427 [Val] Loss: 0.2549607455730438 | Accuracy: 0.9235000610351562\n",
      "Epoch 45 -> [Train] Loss: 0.26115134358406067 | Accuracy: 0.9246500134468079 [Val] Loss: 0.2516634464263916 | Accuracy: 0.9244000315666199\n",
      "Epoch 46 -> [Train] Loss: 0.2575255036354065 | Accuracy: 0.9256666898727417 [Val] Loss: 0.24841749668121338 | Accuracy: 0.9253000617027283\n",
      "Epoch 47 -> [Train] Loss: 0.2540118396282196 | Accuracy: 0.9266166687011719 [Val] Loss: 0.24537406861782074 | Accuracy: 0.926300048828125\n",
      "Epoch 48 -> [Train] Loss: 0.25061294436454773 | Accuracy: 0.9276166558265686 [Val] Loss: 0.24225929379463196 | Accuracy: 0.9277000427246094\n",
      "Epoch 49 -> [Train] Loss: 0.24733121693134308 | Accuracy: 0.928350031375885 [Val] Loss: 0.23935328423976898 | Accuracy: 0.9284000396728516\n",
      "CPU times: user 1min 14s, sys: 12.8 s, total: 1min 27s\n",
      "Wall time: 30.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(config.EPOCHS):\n",
    "    ## Training\n",
    "    for batch in dst_train_rdy.as_numpy_iterator():\n",
    "        state = train_step(state, batch)\n",
    "        # state = compute_metrics(state=state, batch=batch)\n",
    "        # break\n",
    "\n",
    "    ## Log the metrics\n",
    "    for name, value in state.metrics.compute().items():\n",
    "        metrics_history[f\"train_{name}\"].append(value)\n",
    "    \n",
    "    ## Empty the metrics\n",
    "    state = state.replace(metrics=state.metrics.empty())\n",
    "\n",
    "    ## Evaluation\n",
    "    for batch in dst_val_rdy.as_numpy_iterator():\n",
    "        state = compute_metrics(state=state, batch=batch)\n",
    "        # break\n",
    "    for name, value in state.metrics.compute().items():\n",
    "        metrics_history[f\"val_{name}\"].append(value)\n",
    "    state = state.replace(metrics=state.metrics.empty())\n",
    "\n",
    "    ## Checkpointing\n",
    "    if metrics_history[\"val_accuracy\"][-1] >= max(metrics_history[\"val_accuracy\"]):\n",
    "        orbax_checkpointer.save(os.path.join(wandb.run.dir, \"model\"), state, save_args=save_args, force=True) # force=True means allow overwritting.\n",
    "\n",
    "    \n",
    "    wandb.log({\"epoch\": epoch+1, **{name:values[-1] for name, values in metrics_history.items()}})\n",
    "    print(f'Epoch {epoch} -> [Train] Loss: {metrics_history[\"train_loss\"][-1]} | Accuracy: {metrics_history[\"train_accuracy\"][-1]} [Val] Loss: {metrics_history[\"val_loss\"][-1]} | Accuracy: {metrics_history[\"val_accuracy\"][-1]}')\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▄▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>val_loss</td><td>█▆▅▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_accuracy</td><td>0.92835</td></tr><tr><td>train_loss</td><td>0.24733</td></tr><tr><td>val_accuracy</td><td>0.9284</td></tr><tr><td>val_loss</td><td>0.23935</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Single_Forward</strong> at: <a href='https://wandb.ai/jorgvt/MNIST_JAX/runs/u9x7wdmf' target=\"_blank\">https://wandb.ai/jorgvt/MNIST_JAX/runs/u9x7wdmf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230427_125410-u9x7wdmf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the trained model\n",
    "\n",
    "> We have trained our model, let's see if we can load the trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state = create_train_state(Model(), random.PRNGKey(0), optax.adam(config.LEARNING_RATE), input_shape=(1,28,28,1))\n",
    "assert not new_state == state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To restore a saved checkpoint we only have to call the `.restore()` method of the checkpointer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state = orbax_checkpointer.restore(\"test_save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(True, dtype=bool)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state[\"opt_state\"][0][\"count\"] == state.opt_state[0].count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to load the same state but it was loaded as a normal Python `dict`, not as a `TrainState`. If we want to load it as a custom object we have to probide *Orbax* a example of the type of *PyTree* that we want to load. First we'll reinstantiate a new `TrainState` and then we will pass it to `.restore(item=sample_object)` with the `item` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state = create_train_state(Model(), random.PRNGKey(0), optax.adam(config.LEARNING_RATE), input_shape=(1,28,28,1))\n",
    "assert not new_state == state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state = orbax_checkpointer.restore(\"test_save\", item=new_state)\n",
    "assert not new_state == state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we test for equality we will get a `False` result, but that is because the restoration loads the original `jnp.Array` as `np.array`, but their content will be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    Conv_0: {\n",
       "        bias: Array(True, dtype=bool),\n",
       "        kernel: Array(True, dtype=bool),\n",
       "    },\n",
       "    Conv_1: {\n",
       "        bias: Array(True, dtype=bool),\n",
       "        kernel: Array(True, dtype=bool),\n",
       "    },\n",
       "    Dense_0: {\n",
       "        bias: Array(True, dtype=bool),\n",
       "        kernel: Array(True, dtype=bool),\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_util.tree_map(lambda x,y: (x==y).all(), state.params, new_state.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ScaleByAdamState(count=Array(True, dtype=bool), mu=FrozenDict({\n",
       "     Conv_0: {\n",
       "         bias: Array(True, dtype=bool),\n",
       "         kernel: Array(True, dtype=bool),\n",
       "     },\n",
       "     Conv_1: {\n",
       "         bias: Array(True, dtype=bool),\n",
       "         kernel: Array(True, dtype=bool),\n",
       "     },\n",
       "     Dense_0: {\n",
       "         bias: Array(True, dtype=bool),\n",
       "         kernel: Array(True, dtype=bool),\n",
       "     },\n",
       " }), nu=FrozenDict({\n",
       "     Conv_0: {\n",
       "         bias: Array(True, dtype=bool),\n",
       "         kernel: Array(True, dtype=bool),\n",
       "     },\n",
       "     Conv_1: {\n",
       "         bias: Array(True, dtype=bool),\n",
       "         kernel: Array(True, dtype=bool),\n",
       "     },\n",
       "     Dense_0: {\n",
       "         bias: Array(True, dtype=bool),\n",
       "         kernel: Array(True, dtype=bool),\n",
       "     },\n",
       " })),\n",
       " EmptyState())"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_util.tree_map(lambda x,y: (x==y).all(), state.opt_state, new_state.opt_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

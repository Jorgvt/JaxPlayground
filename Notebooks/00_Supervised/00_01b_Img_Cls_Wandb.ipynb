{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification in JAX using `TrainState` (and `WandB`!)\n",
    "\n",
    "> Adding `wandb` logging into the basic example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 17:33:24.463105: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-26 17:33:24.513451: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-26 17:33:26.594757: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], device_type='GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Sequence, Union\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "from jax import lax, random, numpy as jnp\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax import linen as nn\n",
    "from flax import struct\n",
    "from flax.training import train_state\n",
    "\n",
    "import optax\n",
    "\n",
    "from clu import metrics\n",
    "from ml_collections import ConfigDict\n",
    "\n",
    "from einops import reduce\n",
    "import wandb\n",
    "\n",
    "from iqadatasets.datasets import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "\n",
    "> We'll be using MNIST from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (60000,), (10000, 28, 28, 1), (10000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train = X_train[:,:,:,None]/255.0\n",
    "X_test = X_test[:,:,:,None]/255.0\n",
    "Y_train = Y_train.astype(np.int32)\n",
    "Y_test = Y_test.astype(np.int32)\n",
    "\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_train = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "dst_val = tf.data.Dataset.from_tensor_slices((X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BATCH_SIZE: 256\n",
       "EPOCHS: 50\n",
       "LEARNING_RATE: 0.0003"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    \"BATCH_SIZE\": 256,\n",
    "    \"EPOCHS\": 50,\n",
    "    \"LEARNING_RATE\": 3e-4,\n",
    "}\n",
    "config = ConfigDict(config)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjorgvt\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/disk/users/vitojor/JaxPlayground/Notebooks/00_Supervised/wandb/run-20230426_173335-ovzphed8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jorgvt/MNIST_JAX/runs/ovzphed8' target=\"_blank\">Single_Forward</a></strong> to <a href='https://wandb.ai/jorgvt/MNIST_JAX' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jorgvt/MNIST_JAX' target=\"_blank\">https://wandb.ai/jorgvt/MNIST_JAX</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jorgvt/MNIST_JAX/runs/ovzphed8' target=\"_blank\">https://wandb.ai/jorgvt/MNIST_JAX/runs/ovzphed8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BATCH_SIZE: 256\n",
       "EPOCHS: 50\n",
       "LEARNING_RATE: 0.0003"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"MNIST_JAX\",\n",
    "           name=\"Single_Forward\",\n",
    "           job_type=\"training\",\n",
    "           config=config,\n",
    "           mode=\"online\",\n",
    "           )\n",
    "config = config\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_train_rdy = dst_train.batch(config.BATCH_SIZE)\n",
    "dst_val_rdy = dst_val.batch(config.BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model we're going to use\n",
    "\n",
    "> It's going to be a very simple model just for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "                 inputs,\n",
    "                 **kwargs,\n",
    "                 ):\n",
    "        outputs = nn.Conv(features=32, kernel_size=(3,3))(inputs)\n",
    "        outputs = nn.relu(outputs)\n",
    "        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n",
    "        outputs = nn.Conv(features=64, kernel_size=(3,3))(outputs)\n",
    "        outputs = nn.relu(outputs)\n",
    "        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n",
    "        outputs = reduce(outputs, \"b h w c -> b c\", reduction=\"mean\")\n",
    "        outputs = nn.Dense(10)(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the metrics with `clu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "    \"\"\"Collection of metrics to be tracked during training.\"\"\"\n",
    "    accuracy: metrics.Accuracy\n",
    "    loss: metrics.Average.from_output(\"loss\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `TrainState` doesn't include metrics, but it's very easy to subclass it so that it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    metrics: Metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a function that initializes the `TrainState` from a module, a rng key and some optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(module, key, tx, input_shape):\n",
    "    \"\"\"Creates the initial `TrainState`.\"\"\"\n",
    "    params = module.init(key, jnp.ones(input_shape))[\"params\"]\n",
    "    return TrainState.create(\n",
    "        apply_fn=module.apply,\n",
    "        params=params,\n",
    "        tx=tx,\n",
    "        metrics=Metrics.empty()\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the training step\n",
    "\n",
    "> We want to write a function that takes the `TrainState` and a batch of data can performs an optimization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    \"\"\"Train for a single step.\"\"\"\n",
    "    inputs, labels = batch\n",
    "    def loss_fn(params):\n",
    "        pred = state.apply_fn({\"params\": params}, inputs)\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(logits=pred, labels=labels).mean()\n",
    "        return loss, pred\n",
    "    (loss, pred), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    def compute_metrics(state):\n",
    "        metric_updates = state.metrics.single_from_model_output(\n",
    "            logits=pred, labels=labels, loss=loss,\n",
    "        )\n",
    "        metrics = state.metrics.merge(metric_updates)\n",
    "        state = state.replace(metrics=metrics)\n",
    "        return state\n",
    "    state = compute_metrics(state)\n",
    "    return state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In their example, they don't calculate the metrics at the same time. I think it is kind of a waste because it means having to perform a new forward pass, but we'll follow as of now. Let's define a function to perform metric calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def compute_metrics(*, state, batch):\n",
    "    \"\"\"Obtaining the metrics for a given batch.\"\"\"\n",
    "    inputs, labels = batch\n",
    "    pred = state.apply_fn({\"params\": state.params}, inputs)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=pred, labels=labels).mean()\n",
    "    metric_updates = state.metrics.single_from_model_output(\n",
    "        logits=pred, labels=labels, loss=loss,\n",
    "    )\n",
    "    metrics = state.metrics.merge(metric_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = create_train_state(Model(), random.PRNGKey(0), optax.adam(config.LEARNING_RATE), input_shape=(1,28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"train_accuracy\": [],\n",
    "    \"val_accuracy\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 17:33:50.399662: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [60000]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-04-26 17:33:53.381922: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [10000]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 -> [Train] Loss: 2.182008743286133 | Accuracy: 0.2583500146865845 [Val] Loss: 1.936364769935608 | Accuracy: 0.42110002040863037\n",
      "Epoch 1 -> [Train] Loss: 1.6769200563430786 | Accuracy: 0.4977000057697296 [Val] Loss: 1.4756618738174438 | Accuracy: 0.5753000378608704\n",
      "Epoch 2 -> [Train] Loss: 1.3775689601898193 | Accuracy: 0.593250036239624 [Val] Loss: 1.263392448425293 | Accuracy: 0.6458000540733337\n",
      "Epoch 3 -> [Train] Loss: 1.2068145275115967 | Accuracy: 0.6537833213806152 [Val] Loss: 1.1156055927276611 | Accuracy: 0.6897000074386597\n",
      "Epoch 4 -> [Train] Loss: 1.081493616104126 | Accuracy: 0.6955500245094299 [Val] Loss: 1.0011286735534668 | Accuracy: 0.7230000495910645\n",
      "Epoch 5 -> [Train] Loss: 0.9830330014228821 | Accuracy: 0.7260167002677917 [Val] Loss: 0.909456729888916 | Accuracy: 0.751300036907196\n",
      "Epoch 6 -> [Train] Loss: 0.9034460186958313 | Accuracy: 0.7495499849319458 [Val] Loss: 0.8353870511054993 | Accuracy: 0.7705000638961792\n",
      "Epoch 7 -> [Train] Loss: 0.837811291217804 | Accuracy: 0.7681500315666199 [Val] Loss: 0.7737129926681519 | Accuracy: 0.7864000201225281\n",
      "Epoch 8 -> [Train] Loss: 0.7825695872306824 | Accuracy: 0.7839333415031433 [Val] Loss: 0.7215777039527893 | Accuracy: 0.8014000654220581\n",
      "Epoch 9 -> [Train] Loss: 0.7353348731994629 | Accuracy: 0.7968666553497314 [Val] Loss: 0.6768519878387451 | Accuracy: 0.8164000511169434\n",
      "Epoch 10 -> [Train] Loss: 0.6941709518432617 | Accuracy: 0.8088499903678894 [Val] Loss: 0.6378747820854187 | Accuracy: 0.8267000317573547\n",
      "Epoch 11 -> [Train] Loss: 0.6577768921852112 | Accuracy: 0.8183333277702332 [Val] Loss: 0.6033924221992493 | Accuracy: 0.835800051689148\n",
      "Epoch 12 -> [Train] Loss: 0.625186562538147 | Accuracy: 0.8272500038146973 [Val] Loss: 0.572777509689331 | Accuracy: 0.8445000648498535\n",
      "Epoch 13 -> [Train] Loss: 0.5957895517349243 | Accuracy: 0.8355000019073486 [Val] Loss: 0.5452829003334045 | Accuracy: 0.8528000116348267\n",
      "Epoch 14 -> [Train] Loss: 0.5690935850143433 | Accuracy: 0.8436833620071411 [Val] Loss: 0.520549476146698 | Accuracy: 0.8609000444412231\n",
      "Epoch 15 -> [Train] Loss: 0.5447360277175903 | Accuracy: 0.8504000306129456 [Val] Loss: 0.4981409013271332 | Accuracy: 0.8666000366210938\n",
      "Epoch 16 -> [Train] Loss: 0.5224463939666748 | Accuracy: 0.8563166856765747 [Val] Loss: 0.47782716155052185 | Accuracy: 0.8708000183105469\n",
      "Epoch 17 -> [Train] Loss: 0.5019102692604065 | Accuracy: 0.8618000149726868 [Val] Loss: 0.4594748020172119 | Accuracy: 0.8742000460624695\n",
      "Epoch 18 -> [Train] Loss: 0.48303425312042236 | Accuracy: 0.8670666813850403 [Val] Loss: 0.4426192343235016 | Accuracy: 0.8786000609397888\n",
      "Epoch 19 -> [Train] Loss: 0.4656265377998352 | Accuracy: 0.8720833659172058 [Val] Loss: 0.4271697700023651 | Accuracy: 0.8813000321388245\n",
      "Epoch 20 -> [Train] Loss: 0.44959351420402527 | Accuracy: 0.8765333294868469 [Val] Loss: 0.41317644715309143 | Accuracy: 0.8841000199317932\n",
      "Epoch 21 -> [Train] Loss: 0.4347400665283203 | Accuracy: 0.8805333375930786 [Val] Loss: 0.400164932012558 | Accuracy: 0.8879000544548035\n",
      "Epoch 22 -> [Train] Loss: 0.4210180938243866 | Accuracy: 0.8843333721160889 [Val] Loss: 0.38827279210090637 | Accuracy: 0.8905000686645508\n",
      "Epoch 23 -> [Train] Loss: 0.40825262665748596 | Accuracy: 0.8876000046730042 [Val] Loss: 0.3773591220378876 | Accuracy: 0.893500030040741\n",
      "Epoch 24 -> [Train] Loss: 0.39640703797340393 | Accuracy: 0.8908666968345642 [Val] Loss: 0.3671402335166931 | Accuracy: 0.8960000276565552\n",
      "Epoch 25 -> [Train] Loss: 0.3853464722633362 | Accuracy: 0.893500030040741 [Val] Loss: 0.3577060401439667 | Accuracy: 0.8972000479698181\n",
      "Epoch 26 -> [Train] Loss: 0.3750644028186798 | Accuracy: 0.8962500095367432 [Val] Loss: 0.3489246666431427 | Accuracy: 0.8987000584602356\n",
      "Epoch 27 -> [Train] Loss: 0.36541470885276794 | Accuracy: 0.8986333608627319 [Val] Loss: 0.3407832980155945 | Accuracy: 0.9004000425338745\n",
      "Epoch 28 -> [Train] Loss: 0.3564613461494446 | Accuracy: 0.9010833501815796 [Val] Loss: 0.3331397473812103 | Accuracy: 0.902400016784668\n",
      "Epoch 29 -> [Train] Loss: 0.3480415642261505 | Accuracy: 0.902916669845581 [Val] Loss: 0.3259875178337097 | Accuracy: 0.9034000635147095\n",
      "Epoch 30 -> [Train] Loss: 0.34014928340911865 | Accuracy: 0.9047333598136902 [Val] Loss: 0.3192983865737915 | Accuracy: 0.9043000340461731\n",
      "Epoch 31 -> [Train] Loss: 0.3327358663082123 | Accuracy: 0.906333327293396 [Val] Loss: 0.31295037269592285 | Accuracy: 0.9062000513076782\n",
      "Epoch 32 -> [Train] Loss: 0.325710266828537 | Accuracy: 0.9078666567802429 [Val] Loss: 0.30704569816589355 | Accuracy: 0.9079000353813171\n",
      "Epoch 33 -> [Train] Loss: 0.319106787443161 | Accuracy: 0.9093833565711975 [Val] Loss: 0.30143722891807556 | Accuracy: 0.909600019454956\n",
      "Epoch 34 -> [Train] Loss: 0.3128489553928375 | Accuracy: 0.9111666679382324 [Val] Loss: 0.296077698469162 | Accuracy: 0.9110000729560852\n",
      "Epoch 35 -> [Train] Loss: 0.30691418051719666 | Accuracy: 0.9125999808311462 [Val] Loss: 0.2910052239894867 | Accuracy: 0.9123000502586365\n",
      "Epoch 36 -> [Train] Loss: 0.3013036549091339 | Accuracy: 0.9142000079154968 [Val] Loss: 0.28622308373451233 | Accuracy: 0.9141000509262085\n",
      "Epoch 37 -> [Train] Loss: 0.2959589958190918 | Accuracy: 0.9154333472251892 [Val] Loss: 0.28169429302215576 | Accuracy: 0.9154000282287598\n",
      "Epoch 38 -> [Train] Loss: 0.29088759422302246 | Accuracy: 0.9169000387191772 [Val] Loss: 0.27736854553222656 | Accuracy: 0.9163000583648682\n",
      "Epoch 39 -> [Train] Loss: 0.28605058789253235 | Accuracy: 0.9180999994277954 [Val] Loss: 0.273225873708725 | Accuracy: 0.9175000190734863\n",
      "Epoch 40 -> [Train] Loss: 0.2814214527606964 | Accuracy: 0.9194666743278503 [Val] Loss: 0.26928025484085083 | Accuracy: 0.9187000393867493\n",
      "Epoch 41 -> [Train] Loss: 0.277006596326828 | Accuracy: 0.9207333326339722 [Val] Loss: 0.2654680609703064 | Accuracy: 0.9199000597000122\n",
      "Epoch 42 -> [Train] Loss: 0.2727579176425934 | Accuracy: 0.9218500256538391 [Val] Loss: 0.2618121802806854 | Accuracy: 0.921000063419342\n",
      "Epoch 43 -> [Train] Loss: 0.26871034502983093 | Accuracy: 0.9228166937828064 [Val] Loss: 0.25826191902160645 | Accuracy: 0.9226000308990479\n",
      "Epoch 44 -> [Train] Loss: 0.26478293538093567 | Accuracy: 0.9237833619117737 [Val] Loss: 0.25480398535728455 | Accuracy: 0.9236000180244446\n",
      "Epoch 45 -> [Train] Loss: 0.26100045442581177 | Accuracy: 0.9246333241462708 [Val] Loss: 0.2515222132205963 | Accuracy: 0.924500048160553\n",
      "Epoch 46 -> [Train] Loss: 0.2573975622653961 | Accuracy: 0.9256166815757751 [Val] Loss: 0.24827086925506592 | Accuracy: 0.9252000451087952\n",
      "Epoch 47 -> [Train] Loss: 0.2538735866546631 | Accuracy: 0.9266499876976013 [Val] Loss: 0.24520094692707062 | Accuracy: 0.9265000224113464\n",
      "Epoch 48 -> [Train] Loss: 0.2504808008670807 | Accuracy: 0.9275833368301392 [Val] Loss: 0.24217760562896729 | Accuracy: 0.9275000691413879\n",
      "Epoch 49 -> [Train] Loss: 0.24721772968769073 | Accuracy: 0.9284166693687439 [Val] Loss: 0.23932170867919922 | Accuracy: 0.9280000329017639\n",
      "CPU times: user 1min 7s, sys: 10.7 s, total: 1min 18s\n",
      "Wall time: 26.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(config.EPOCHS):\n",
    "    ## Training\n",
    "    for batch in dst_train_rdy.as_numpy_iterator():\n",
    "        state = train_step(state, batch)\n",
    "        # state = compute_metrics(state=state, batch=batch)\n",
    "        # break\n",
    "\n",
    "    ## Log the metrics\n",
    "    for name, value in state.metrics.compute().items():\n",
    "        metrics_history[f\"train_{name}\"].append(value)\n",
    "    \n",
    "    ## Empty the metrics\n",
    "    state = state.replace(metrics=state.metrics.empty())\n",
    "\n",
    "    ## Evaluation\n",
    "    for batch in dst_val_rdy.as_numpy_iterator():\n",
    "        state = compute_metrics(state=state, batch=batch)\n",
    "        # break\n",
    "    for name, value in state.metrics.compute().items():\n",
    "        metrics_history[f\"val_{name}\"].append(value)\n",
    "    state = state.replace(metrics=state.metrics.empty())\n",
    "    \n",
    "    wandb.log({\"epoch\": epoch+1, **{name:values[-1] for name, values in metrics_history.items()}})\n",
    "    print(f'Epoch {epoch} -> [Train] Loss: {metrics_history[\"train_loss\"][-1]} | Accuracy: {metrics_history[\"train_accuracy\"][-1]} [Val] Loss: {metrics_history[\"val_loss\"][-1]} | Accuracy: {metrics_history[\"val_accuracy\"][-1]}')\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>▁▄▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇██████████████████████</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>val_loss</td><td>█▆▅▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>0.92842</td></tr><tr><td>train_loss</td><td>0.24722</td></tr><tr><td>val_accuracy</td><td>0.928</td></tr><tr><td>val_loss</td><td>0.23932</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Single_Forward</strong> at: <a href='https://wandb.ai/jorgvt/MNIST_JAX/runs/ovzphed8' target=\"_blank\">https://wandb.ai/jorgvt/MNIST_JAX/runs/ovzphed8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230426_173335-ovzphed8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

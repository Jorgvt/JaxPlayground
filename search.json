[
  {
    "objectID": "02_Generative/00_vae.html",
    "href": "02_Generative/00_vae.html",
    "title": "Variational Autoencoder implemented in JaX!",
    "section": "",
    "text": "import tensorflow as tf\ntf.config.set_visible_devices([], device_type='GPU')\n\n2023-05-01 13:46:43.807364: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2023-05-01 13:46:43.860179: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-05-01 13:46:46.020888: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nfrom typing import Any, Callable, Sequence, Union\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport jax\nfrom jax import lax, random, numpy as jnp\nfrom flax.core import freeze, unfreeze, FrozenDict\nfrom flax import linen as nn\nfrom flax import struct\nfrom flax.training import train_state\n\nimport optax\n\nfrom clu import metrics\nfrom ml_collections import ConfigDict\n\nfrom einops import rearrange, reduce\n\nfrom iqadatasets.datasets import *\nfrom fxlayers.layers import *"
  },
  {
    "objectID": "02_Generative/00_vae.html#get-some-data",
    "href": "02_Generative/00_vae.html#get-some-data",
    "title": "Variational Autoencoder implemented in JaX!",
    "section": "Get some data",
    "text": "Get some data\n\nThe interesting part is going to be the model/training definition, so we’ll use MNIST to keep it as simple as possible.\n\n\n(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.mnist.load_data()\n\nX_train = X_train[:,:,:,None]/255.0\nX_test = X_test[:,:,:,None]/255.0\nY_train = Y_train.astype(np.int32)\nY_test = Y_test.astype(np.int32)\n\nX_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n\n((60000, 28, 28, 1), (60000,), (10000, 28, 28, 1), (10000,))\n\n\n\ndst_train = tf.data.Dataset.from_tensor_slices((X_train))\ndst_val = tf.data.Dataset.from_tensor_slices((X_test))\n\n\nconfig = {\n    \"BATCH_SIZE\": 32,\n    \"EPOCHS\": 5,\n    \"LEARNING_RATE\": 3e-4,\n}\nconfig = ConfigDict(config)\nconfig\n\nBATCH_SIZE: 32\nEPOCHS: 5\nLEARNING_RATE: 0.0003\n\n\n\ndst_train_rdy = dst_train.batch(config.BATCH_SIZE)\ndst_val_rdy = dst_val.batch(config.BATCH_SIZE)"
  },
  {
    "objectID": "02_Generative/00_vae.html#define-the-model-were-going-to-use",
    "href": "02_Generative/00_vae.html#define-the-model-were-going-to-use",
    "title": "Variational Autoencoder implemented in JaX!",
    "section": "Define the model we’re going to use",
    "text": "Define the model we’re going to use\n\nAs we want to be able to encode and decode images separatelly, we should define the encoder and decoder modules.\n\n\nclass Encoder(nn.Module):\n    \"\"\"CVAE Encoder\"\"\"\n    latent_dim: int = 2\n\n    @nn.compact\n    def __call__(self,\n                 inputs,\n                 **kwargs,\n                 ):\n        \"\"\"Takes an image as inputs and encodes it as a mean and logvar.\"\"\"\n        outputs = nn.Conv(features=32, kernel_size=(5,5), strides=1, padding=\"SAME\")(inputs)\n        outputs = nn.relu(outputs)\n        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n        outputs = nn.Conv(features=64, kernel_size=(3,3), strides=1, padding=\"SAME\")(outputs)\n        outputs = nn.relu(outputs)\n        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n        outputs = rearrange(outputs, \"b h w c -&gt; b (h w c)\")\n        mean, logvar = nn.Dense(features=self.latent_dim)(outputs), nn.Dense(features=self.latent_dim)(outputs)\n        return mean, logvar\n\n\nclass Decoder(nn.Module):\n    \"\"\"CVAE Encoder\"\"\"\n    \n    @nn.compact\n    def __call__(self,\n                 inputs,\n                 **kwargs,\n                 ):\n        \"\"\"Takes a resampled point and outputs an image.\"\"\"\n        outputs = nn.Dense(features=3136)(inputs)\n        outputs = nn.relu(outputs)\n        outputs = rearrange(outputs, \"b (h w c) -&gt; b h w c\", h=7, w=7, c=64)\n        outputs = nn.ConvTranspose(features=32, kernel_size=(3,3), strides=(2,2), padding=\"SAME\")(outputs)\n        outputs = nn.relu(outputs)\n        outputs = nn.ConvTranspose(features=1, kernel_size=(5,5), strides=(2,2), padding=\"SAME\")(outputs)\n        outputs = nn.relu(outputs)\n        return outputs\n\n\nclass CVAE(nn.Module):\n    \"\"\"Convolutional Variational Autoencoder\"\"\"\n    latent_dim: int = 2\n\n    def setup(self):\n        self.encoder = Encoder(latent_dim=self.latent_dim)\n        self.decoder = Decoder()\n\n    def __call__(self,\n                 inputs,\n                 key=random.PRNGKey(0),\n                 train=True,\n                 **kwargs,\n                 ):\n        mean, logvar = self.encoder(inputs)\n        z = self.resample(key, mean, logvar)\n        outputs = self.decoder(z)\n        ## We have to return the mean and logvar to calculate the loss during training\n        ## but we don't need them during validation/testing.\n        # return jnp.where(train, jnp.array([outputs, mean, logvar]), outputs)\n        return outputs, mean, logvar\n    \n    @staticmethod\n    def resample(key,\n                 mean,\n                 logvar,\n                 ):\n        return mean + jnp.exp(0.5*logvar)*random.normal(key, shape=logvar.shape)"
  },
  {
    "objectID": "02_Generative/00_vae.html#define-the-metrics-with-clu",
    "href": "02_Generative/00_vae.html#define-the-metrics-with-clu",
    "title": "Variational Autoencoder implemented in JaX!",
    "section": "Define the metrics with clu",
    "text": "Define the metrics with clu\n\n@struct.dataclass\nclass Metrics(metrics.Collection):\n    \"\"\"Collection of metrics to be tracked during training.\"\"\"\n    total_loss: metrics.Average.from_output(\"total_loss\")\n    reconstruction_loss: metrics.Average.from_output(\"reconstruction_loss\")\n    kl_div: metrics.Average.from_output(\"kl_div\")\n\nBy default, TrainState doesn’t include metrics, but it’s very easy to subclass it so that it does:\n\nclass TrainState(train_state.TrainState):\n    metrics: Metrics\n\nWe’ll define a function that initializes the TrainState from a module, a rng key and some optimizer:\n\ndef create_train_state(module, key, tx, input_shape):\n    \"\"\"Creates the initial `TrainState`.\"\"\"\n    params = module.init(key, jnp.ones(input_shape))[\"params\"]\n    return TrainState.create(\n        apply_fn=module.apply,\n        params=params,\n        tx=tx,\n        metrics=Metrics.empty()\n    )"
  },
  {
    "objectID": "02_Generative/00_vae.html#defining-the-training-step",
    "href": "02_Generative/00_vae.html#defining-the-training-step",
    "title": "Variational Autoencoder implemented in JaX!",
    "section": "Defining the training step",
    "text": "Defining the training step\n\nWe want to write a function that takes the TrainState and a batch of data can performs an optimization step.\n\nWe are going to define the KL Divergence ourselves (because it has a closed form when comparing two Normal distributions).\n\nSee that we define it for a single sample but we will vmap it after.\n\n\ndef kl_divergence(mean, logvar):\n  return -0.5 * jnp.sum(1 + logvar - jnp.square(mean) - jnp.exp(logvar))\n\n\n@jax.jit\ndef train_step(state, batch):\n    \"\"\"Train for a single step.\"\"\"\n    inputs = batch\n    def loss_fn(params):\n        pred, mean, logvar = state.apply_fn({\"params\": params}, inputs)\n        reconstruction_loss = optax.l2_loss(predictions=pred, targets=inputs).mean()\n        kl_loss = jax.vmap(kl_divergence, in_axes=(0,0))(mean, logvar).mean()\n        total_loss = reconstruction_loss + kl_loss\n        return total_loss, (pred, reconstruction_loss, kl_loss)\n    (total_loss, (pred, reconstruction_loss, kl_loss)), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n    state = state.apply_gradients(grads=grads)\n    def compute_metrics(state):\n        metric_updates = state.metrics.single_from_model_output(\n            total_loss=total_loss, reconstruction_loss=reconstruction_loss, kl_div=kl_loss\n        )\n        metrics = state.metrics.merge(metric_updates)\n        state = state.replace(metrics=metrics)\n        return state\n    state = compute_metrics(state)\n    return state\n\n\n@jax.jit\ndef compute_metrics(*, state, batch):\n    \"\"\"Compute metrics for a single batch.\"\"\"\n    inputs = batch\n    def loss_fn(params):\n        pred, mean, logvar = state.apply_fn({\"params\": params}, inputs)\n        reconstruction_loss = optax.l2_loss(predictions=pred, targets=inputs).mean()\n        kl_loss = jax.vmap(kl_divergence, in_axes=(0,0))(mean, logvar).mean()\n        total_loss = reconstruction_loss + kl_loss\n        return total_loss, (pred, reconstruction_loss, kl_loss)\n    total_loss, (pred, reconstruction_loss, kl_loss) = loss_fn(state.params)\n    def _compute_metrics(state):\n        metric_updates = state.metrics.single_from_model_output(\n            total_loss=total_loss, reconstruction_loss=reconstruction_loss, kl_div=kl_loss\n        )\n        metrics = state.metrics.merge(metric_updates)\n        state = state.replace(metrics=metrics)\n        return state\n    state = _compute_metrics(state)\n    return state"
  },
  {
    "objectID": "02_Generative/00_vae.html#train-the-model",
    "href": "02_Generative/00_vae.html#train-the-model",
    "title": "Variational Autoencoder implemented in JaX!",
    "section": "Train the model!",
    "text": "Train the model!\n\nstate = create_train_state(CVAE(), random.PRNGKey(0), optax.adam(config.LEARNING_RATE), input_shape=(1,28,28,1))\n\n\nmetrics_history = {\n    \"train_total_loss\": [],\n    \"val_total_loss\": [],\n    \"train_reconstruction_loss\": [],\n    \"val_reconstruction_loss\": [],\n    \"train_kl_div\": [],\n    \"val_kl_div\": [],\n}\n\n\nfor epoch in range(config.EPOCHS):\n    ## Training\n    for batch in dst_train_rdy.as_numpy_iterator():\n        state = train_step(state, batch)\n        # state = compute_metrics(state=state, batch=batch)\n        # break\n\n    ## Log the metrics\n    for name, value in state.metrics.compute().items():\n        metrics_history[f\"train_{name}\"].append(value)\n    \n    ## Empty the metrics\n    state = state.replace(metrics=state.metrics.empty())\n\n    ## Evaluation\n    for batch in dst_val_rdy.as_numpy_iterator():\n        state = compute_metrics(state=state, batch=batch)\n        # break\n    for name, value in state.metrics.compute().items():\n        metrics_history[f\"val_{name}\"].append(value)\n    state = state.replace(metrics=state.metrics.empty())\n    \n    # wandb.log({\"epoch\": epoch+1, **{name:values[-1] for name, values in metrics_history.items()}})\n    print(f'Epoch {epoch} -&gt; [Train] Total Loss: {metrics_history[\"train_total_loss\"][-1]} | Reconstruction: {metrics_history[\"train_reconstruction_loss\"][-1]} | KL: {metrics_history[\"train_kl_div\"][-1]} [Val] Total Loss: {metrics_history[\"val_total_loss\"][-1]} | Reconstruction: {metrics_history[\"val_reconstruction_loss\"][-1]} | KL: {metrics_history[\"val_kl_div\"][-1]}')\n    # break\n\nEpoch 0 -&gt; [Train] Total Loss: 0.03677105903625488 | Reconstruction: 0.03485780954360962 | KL: 0.0019132716115564108 [Val] Total Loss: 0.03431036323308945 | Reconstruction: 0.03427962586283684 | KL: 3.073074185522273e-05\nEpoch 1 -&gt; [Train] Total Loss: 0.03368878364562988 | Reconstruction: 0.03368175029754639 | KL: 7.0610335569654126e-06 [Val] Total Loss: 0.034519486129283905 | Reconstruction: 0.03451821208000183 | KL: 1.2732785990010598e-06\nEpoch 2 -&gt; [Train] Total Loss: 0.033580709248781204 | Reconstruction: 0.03358020633459091 | KL: 5.684336201738915e-07 [Val] Total Loss: 0.03442131355404854 | Reconstruction: 0.03442039340734482 | KL: 9.191528533847304e-07\nEpoch 3 -&gt; [Train] Total Loss: 0.03352777287364006 | Reconstruction: 0.0335274338722229 | KL: 3.036886369045533e-07 [Val] Total Loss: 0.034319933503866196 | Reconstruction: 0.03431900963187218 | KL: 9.296860525864759e-07\nEpoch 4 -&gt; [Train] Total Loss: 0.03349529206752777 | Reconstruction: 0.0334949791431427 | KL: 3.5220733707319596e-07 [Val] Total Loss: 0.034218478947877884 | Reconstruction: 0.03421798720955849 | KL: 5.371261408981809e-07\nCPU times: user 27.1 s, sys: 1.6 s, total: 28.7 s\nWall time: 14.8 s"
  },
  {
    "objectID": "02_Generative/00_vae.html#generate-samples",
    "href": "02_Generative/00_vae.html#generate-samples",
    "title": "Variational Autoencoder implemented in JaX!",
    "section": "Generate samples!",
    "text": "Generate samples!\n\nTo generate samples we have to generate normal distributed numbers and pass them through the decoder of the model.\n\n\nfrom functools import partial\n\n\npartial(jax.jit, static_argnums=(2))\ndef generate(key, n, params, latent_dim=2):\n    latents = random.normal(key, shape=(n,latent_dim))\n    pred = Decoder().apply(params, latents)\n    return pred\n\n\ng = generate(random.PRNGKey(0), 10, {\"params\": state.params[\"decoder\"]}, latent_dim=2)\n\nCPU times: user 42 µs, sys: 12 µs, total: 54 µs\nWall time: 107 µs\n\n\n\nfig, axes = plt.subplots(1,10)\nfor img, ax in zip(g, axes):\n    ax.imshow(img)\n    ax.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "00_Supervised/01_image_classification_trainstate.html",
    "href": "00_Supervised/01_image_classification_trainstate.html",
    "title": "Image Classification in JAX using TrainState",
    "section": "",
    "text": "import tensorflow as tf\ntf.config.set_visible_devices([], device_type='GPU')\n\n2023-04-26 12:28:31.204444: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2023-04-26 12:28:31.258188: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-04-26 12:28:33.023032: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nfrom typing import Any, Callable, Sequence, Union\nimport numpy as np\n\nimport jax\nfrom jax import lax, random, numpy as jnp\nfrom flax.core import freeze, unfreeze\nfrom flax import linen as nn\nfrom flax import struct\nfrom flax.training import train_state\n\nimport optax\n\nfrom clu import metrics\nfrom ml_collections import ConfigDict\n\nfrom einops import reduce\n\nfrom iqadatasets.datasets import *"
  },
  {
    "objectID": "00_Supervised/01_image_classification_trainstate.html#get-the-data",
    "href": "00_Supervised/01_image_classification_trainstate.html#get-the-data",
    "title": "Image Classification in JAX using TrainState",
    "section": "Get the data",
    "text": "Get the data\n\nWe’ll be using MNIST from Keras.\n\n\n(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.mnist.load_data()\n\nX_train = X_train[:,:,:,None]/255.0\nX_test = X_test[:,:,:,None]/255.0\nY_train = Y_train.astype(np.int32)\nY_test = Y_test.astype(np.int32)\n\nX_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n\n((60000, 28, 28, 1), (60000,), (10000, 28, 28, 1), (10000,))\n\n\n\ndst_train = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\ndst_val = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n\n\nconfig = {\n    \"BATCH_SIZE\": 256,\n    \"EPOCHS\": 50,\n    \"LEARNING_RATE\": 3e-4,\n}\nconfig = ConfigDict(config)\nconfig\n\nBATCH_SIZE: 256\nEPOCHS: 50\nLEARNING_RATE: 0.0003\n\n\n\ndst_train_rdy = dst_train.batch(config.BATCH_SIZE)\ndst_val_rdy = dst_val.batch(config.BATCH_SIZE)"
  },
  {
    "objectID": "00_Supervised/01_image_classification_trainstate.html#define-the-model-were-going-to-use",
    "href": "00_Supervised/01_image_classification_trainstate.html#define-the-model-were-going-to-use",
    "title": "Image Classification in JAX using TrainState",
    "section": "Define the model we’re going to use",
    "text": "Define the model we’re going to use\n\nIt’s going to be a very simple model just for demonstration purposes.\n\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self,\n                 inputs,\n                 **kwargs,\n                 ):\n        outputs = nn.Conv(features=32, kernel_size=(3,3))(inputs)\n        outputs = nn.relu(outputs)\n        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n        outputs = nn.Conv(features=64, kernel_size=(3,3))(outputs)\n        outputs = nn.relu(outputs)\n        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n        outputs = reduce(outputs, \"b h w c -&gt; b c\", reduction=\"mean\")\n        outputs = nn.Dense(10)(outputs)\n        return outputs"
  },
  {
    "objectID": "00_Supervised/01_image_classification_trainstate.html#define-the-metrics-with-clu",
    "href": "00_Supervised/01_image_classification_trainstate.html#define-the-metrics-with-clu",
    "title": "Image Classification in JAX using TrainState",
    "section": "Define the metrics with clu",
    "text": "Define the metrics with clu\n\n@struct.dataclass\nclass Metrics(metrics.Collection):\n    \"\"\"Collection of metrics to be tracked during training.\"\"\"\n    accuracy: metrics.Accuracy\n    loss: metrics.Average.from_output(\"loss\")\n\nBy default, TrainState doesn’t include metrics, but it’s very easy to subclass it so that it does:\n\nclass TrainState(train_state.TrainState):\n    metrics: Metrics\n\nWe’ll define a function that initializes the TrainState from a module, a rng key and some optimizer:\n\ndef create_train_state(module, key, tx, input_shape):\n    \"\"\"Creates the initial `TrainState`.\"\"\"\n    params = module.init(key, jnp.ones(input_shape))[\"params\"]\n    return TrainState.create(\n        apply_fn=module.apply,\n        params=params,\n        tx=tx,\n        metrics=Metrics.empty()\n    )"
  },
  {
    "objectID": "00_Supervised/01_image_classification_trainstate.html#defining-the-training-step",
    "href": "00_Supervised/01_image_classification_trainstate.html#defining-the-training-step",
    "title": "Image Classification in JAX using TrainState",
    "section": "Defining the training step",
    "text": "Defining the training step\n\nWe want to write a function that takes the TrainState and a batch of data can performs an optimization step.\n\n\n@jax.jit\ndef train_step(state, batch):\n    \"\"\"Train for a single step.\"\"\"\n    inputs, labels = batch\n    def loss(params):\n        pred = state.apply_fn({\"params\": params}, inputs)\n        loss = optax.softmax_cross_entropy_with_integer_labels(logits=pred, labels=labels).mean()\n        return loss\n    grads = jax.grad(loss)(state.params)\n    state = state.apply_gradients(grads=grads)\n    return state\n\nIn their example, they don’t calculate the metrics at the same time. I think it is kind of a waste because it means having to perform a new forward pass, but we’ll follow as of now. Let’s define a function to perform metric calculation:\n\n@jax.jit\ndef compute_metrics(*, state, batch):\n    \"\"\"Obtaining the metrics for a given batch.\"\"\"\n    inputs, labels = batch\n    pred = state.apply_fn({\"params\": state.params}, inputs)\n    loss = optax.softmax_cross_entropy_with_integer_labels(logits=pred, labels=labels).mean()\n    metric_updates = state.metrics.single_from_model_output(\n        logits=pred, labels=labels, loss=loss,\n    )\n    metrics = state.metrics.merge(metric_updates)\n    state = state.replace(metrics=metrics)\n    return state"
  },
  {
    "objectID": "00_Supervised/01_image_classification_trainstate.html#train-the-model",
    "href": "00_Supervised/01_image_classification_trainstate.html#train-the-model",
    "title": "Image Classification in JAX using TrainState",
    "section": "Train the model!",
    "text": "Train the model!\n\nstate = create_train_state(Model(), random.PRNGKey(0), optax.adam(config.LEARNING_RATE), input_shape=(1,28,28,1))\n\n\nmetrics_history = {\n    \"train_loss\": [],\n    \"val_loss\": [],\n    \"train_accuracy\": [],\n    \"val_accuracy\": [],\n}\n\n\nfor epoch in range(config.EPOCHS):\n    ## Training\n    for batch in dst_train_rdy.as_numpy_iterator():\n        state = train_step(state, batch)\n        state = compute_metrics(state=state, batch=batch)\n        # break\n\n    ## Log the metrics\n    for name, value in state.metrics.compute().items():\n        metrics_history[f\"train_{name}\"].append(value)\n    \n    ## Empty the metrics\n    state = state.replace(metrics=state.metrics.empty())\n\n    ## Evaluation\n    for batch in dst_val_rdy.as_numpy_iterator():\n        state = compute_metrics(state=state, batch=batch)\n        # break\n    for name, value in state.metrics.compute().items():\n        metrics_history[f\"val_{name}\"].append(value)\n    state = state.replace(metrics=state.metrics.empty())\n    \n    print(f'Epoch {epoch} -&gt; [Train] Loss: {metrics_history[\"train_loss\"][-1]} | Accuracy: {metrics_history[\"train_accuracy\"][-1]} [Val] Loss: {metrics_history[\"val_loss\"][-1]} | Accuracy: {metrics_history[\"val_accuracy\"][-1]}')\n    # break\n\nEpoch 0 -&gt; [Train] Loss: 2.179863691329956 | Accuracy: 0.26034998893737793 [Val] Loss: 1.9363536834716797 | Accuracy: 0.4207000136375427\nEpoch 1 -&gt; [Train] Loss: 1.6733933687210083 | Accuracy: 0.501966655254364 [Val] Loss: 1.4757570028305054 | Accuracy: 0.5743000507354736\nEpoch 2 -&gt; [Train] Loss: 1.3742271661758423 | Accuracy: 0.5971333384513855 [Val] Loss: 1.263370394706726 | Accuracy: 0.6460000276565552\nEpoch 3 -&gt; [Train] Loss: 1.2035051584243774 | Accuracy: 0.6574167013168335 [Val] Loss: 1.11570143699646 | Accuracy: 0.6895000338554382\nEpoch 4 -&gt; [Train] Loss: 1.0782545804977417 | Accuracy: 0.6978999972343445 [Val] Loss: 1.0012279748916626 | Accuracy: 0.7230000495910645\nEpoch 5 -&gt; [Train] Loss: 0.9798782467842102 | Accuracy: 0.7282666563987732 [Val] Loss: 0.9096516966819763 | Accuracy: 0.7509000301361084\nEpoch 6 -&gt; [Train] Loss: 0.9003434777259827 | Accuracy: 0.7511667013168335 [Val] Loss: 0.8355976343154907 | Accuracy: 0.7704000473022461\nEpoch 7 -&gt; [Train] Loss: 0.8347769975662231 | Accuracy: 0.7699000239372253 [Val] Loss: 0.7737947702407837 | Accuracy: 0.7861000299453735\nEpoch 8 -&gt; [Train] Loss: 0.7795228362083435 | Accuracy: 0.7852333188056946 [Val] Loss: 0.7216793894767761 | Accuracy: 0.8015000224113464\nEpoch 9 -&gt; [Train] Loss: 0.7322704195976257 | Accuracy: 0.7983333468437195 [Val] Loss: 0.6768896579742432 | Accuracy: 0.8161000609397888\nEpoch 10 -&gt; [Train] Loss: 0.6911123394966125 | Accuracy: 0.8096333146095276 [Val] Loss: 0.6379691958427429 | Accuracy: 0.8264000415802002\nEpoch 11 -&gt; [Train] Loss: 0.6547446846961975 | Accuracy: 0.8193166851997375 [Val] Loss: 0.6034817099571228 | Accuracy: 0.835800051689148\nEpoch 12 -&gt; [Train] Loss: 0.6221888065338135 | Accuracy: 0.8286833167076111 [Val] Loss: 0.5728276371955872 | Accuracy: 0.8444000482559204\nEpoch 13 -&gt; [Train] Loss: 0.592809796333313 | Accuracy: 0.8369500041007996 [Val] Loss: 0.545353353023529 | Accuracy: 0.8529000282287598\nEpoch 14 -&gt; [Train] Loss: 0.5661748647689819 | Accuracy: 0.8441833257675171 [Val] Loss: 0.5206612348556519 | Accuracy: 0.8607000112533569\nEpoch 15 -&gt; [Train] Loss: 0.5418640971183777 | Accuracy: 0.8511666655540466 [Val] Loss: 0.49828776717185974 | Accuracy: 0.8664000630378723\nEpoch 16 -&gt; [Train] Loss: 0.5196174383163452 | Accuracy: 0.8570833206176758 [Val] Loss: 0.4779704213142395 | Accuracy: 0.8706000447273254\nEpoch 17 -&gt; [Train] Loss: 0.4991525709629059 | Accuracy: 0.8631166815757751 [Val] Loss: 0.4595378041267395 | Accuracy: 0.8741000294685364\nEpoch 18 -&gt; [Train] Loss: 0.4803047478199005 | Accuracy: 0.8684833645820618 [Val] Loss: 0.44265756011009216 | Accuracy: 0.8782000541687012\nEpoch 19 -&gt; [Train] Loss: 0.4629392623901367 | Accuracy: 0.8734166622161865 [Val] Loss: 0.4272184371948242 | Accuracy: 0.8815000653266907\nEpoch 20 -&gt; [Train] Loss: 0.44692835211753845 | Accuracy: 0.8776833415031433 [Val] Loss: 0.41302505135536194 | Accuracy: 0.8848000168800354\nEpoch 21 -&gt; [Train] Loss: 0.4320833683013916 | Accuracy: 0.8814666867256165 [Val] Loss: 0.4000661075115204 | Accuracy: 0.8880000710487366\nEpoch 22 -&gt; [Train] Loss: 0.41831517219543457 | Accuracy: 0.884850025177002 [Val] Loss: 0.38810011744499207 | Accuracy: 0.8910000324249268\nEpoch 23 -&gt; [Train] Loss: 0.40555620193481445 | Accuracy: 0.8883000016212463 [Val] Loss: 0.37713727355003357 | Accuracy: 0.8941000699996948\nEpoch 24 -&gt; [Train] Loss: 0.3936755955219269 | Accuracy: 0.8917999863624573 [Val] Loss: 0.3669348657131195 | Accuracy: 0.8962000608444214\nEpoch 25 -&gt; [Train] Loss: 0.38261884450912476 | Accuracy: 0.8945500254631042 [Val] Loss: 0.35752519965171814 | Accuracy: 0.8974000215530396\nEpoch 26 -&gt; [Train] Loss: 0.3723006844520569 | Accuracy: 0.897016704082489 [Val] Loss: 0.3487200438976288 | Accuracy: 0.8986000418663025\nEpoch 27 -&gt; [Train] Loss: 0.36266595125198364 | Accuracy: 0.8993666768074036 [Val] Loss: 0.34057819843292236 | Accuracy: 0.9010000228881836\nEpoch 28 -&gt; [Train] Loss: 0.35366111993789673 | Accuracy: 0.9014166593551636 [Val] Loss: 0.3329307734966278 | Accuracy: 0.9025000333786011\nEpoch 29 -&gt; [Train] Loss: 0.34524428844451904 | Accuracy: 0.9036999940872192 [Val] Loss: 0.325786292552948 | Accuracy: 0.9038000702857971\nEpoch 30 -&gt; [Train] Loss: 0.3373560309410095 | Accuracy: 0.9056666493415833 [Val] Loss: 0.3190995752811432 | Accuracy: 0.9052000641822815\nEpoch 31 -&gt; [Train] Loss: 0.3299282193183899 | Accuracy: 0.9071666598320007 [Val] Loss: 0.31279483437538147 | Accuracy: 0.9064000248908997\nEpoch 32 -&gt; [Train] Loss: 0.3229246437549591 | Accuracy: 0.9087333679199219 [Val] Loss: 0.3067791163921356 | Accuracy: 0.9082000255584717\nEpoch 33 -&gt; [Train] Loss: 0.316310852766037 | Accuracy: 0.9103000164031982 [Val] Loss: 0.3012504279613495 | Accuracy: 0.9094000458717346\nEpoch 34 -&gt; [Train] Loss: 0.3100613057613373 | Accuracy: 0.9118666648864746 [Val] Loss: 0.29597610235214233 | Accuracy: 0.9111000299453735\nEpoch 35 -&gt; [Train] Loss: 0.30415719747543335 | Accuracy: 0.913349986076355 [Val] Loss: 0.2908722162246704 | Accuracy: 0.9120000600814819\nEpoch 36 -&gt; [Train] Loss: 0.2985559105873108 | Accuracy: 0.9148666858673096 [Val] Loss: 0.2861136496067047 | Accuracy: 0.9140000343322754\nEpoch 37 -&gt; [Train] Loss: 0.2932088375091553 | Accuracy: 0.9163833260536194 [Val] Loss: 0.28155517578125 | Accuracy: 0.9150000214576721\nEpoch 38 -&gt; [Train] Loss: 0.2881231904029846 | Accuracy: 0.9179166555404663 [Val] Loss: 0.2772302031517029 | Accuracy: 0.9162000417709351\nEpoch 39 -&gt; [Train] Loss: 0.2832687199115753 | Accuracy: 0.9191666841506958 [Val] Loss: 0.2730538249015808 | Accuracy: 0.9175000190734863\nEpoch 40 -&gt; [Train] Loss: 0.2786351144313812 | Accuracy: 0.9204166531562805 [Val] Loss: 0.26905083656311035 | Accuracy: 0.9188000559806824\nEpoch 41 -&gt; [Train] Loss: 0.2742093503475189 | Accuracy: 0.9217333197593689 [Val] Loss: 0.26526781916618347 | Accuracy: 0.9198000431060791\nEpoch 42 -&gt; [Train] Loss: 0.26996535062789917 | Accuracy: 0.9227166771888733 [Val] Loss: 0.26161062717437744 | Accuracy: 0.921000063419342\nEpoch 43 -&gt; [Train] Loss: 0.26588964462280273 | Accuracy: 0.9237833619117737 [Val] Loss: 0.258030503988266 | Accuracy: 0.9226000308990479\nEpoch 44 -&gt; [Train] Loss: 0.26196348667144775 | Accuracy: 0.9247666597366333 [Val] Loss: 0.25455671548843384 | Accuracy: 0.9237000346183777\nEpoch 45 -&gt; [Train] Loss: 0.25818783044815063 | Accuracy: 0.9259999990463257 [Val] Loss: 0.2512337863445282 | Accuracy: 0.9246000647544861\nEpoch 46 -&gt; [Train] Loss: 0.2545514404773712 | Accuracy: 0.926800012588501 [Val] Loss: 0.24799060821533203 | Accuracy: 0.9257000684738159\nEpoch 47 -&gt; [Train] Loss: 0.2510388195514679 | Accuracy: 0.9277666807174683 [Val] Loss: 0.24487821757793427 | Accuracy: 0.9269000291824341\nEpoch 48 -&gt; [Train] Loss: 0.24765527248382568 | Accuracy: 0.9287333488464355 [Val] Loss: 0.24188826978206635 | Accuracy: 0.9278000593185425\nEpoch 49 -&gt; [Train] Loss: 0.24437624216079712 | Accuracy: 0.9296333193778992 [Val] Loss: 0.2388812154531479 | Accuracy: 0.9284000396728516\nCPU times: user 1min 14s, sys: 9.94 s, total: 1min 24s\nWall time: 29.3 s"
  },
  {
    "objectID": "00_Supervised/01b_img_cls_wandb.html",
    "href": "00_Supervised/01b_img_cls_wandb.html",
    "title": "Image Classification in JAX using TrainState (and WandB!)",
    "section": "",
    "text": "import tensorflow as tf\ntf.config.set_visible_devices([], device_type='GPU')\n\n2023-04-26 17:33:24.463105: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2023-04-26 17:33:24.513451: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-04-26 17:33:26.594757: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nfrom typing import Any, Callable, Sequence, Union\nimport numpy as np\n\nimport jax\nfrom jax import lax, random, numpy as jnp\nfrom flax.core import freeze, unfreeze\nfrom flax import linen as nn\nfrom flax import struct\nfrom flax.training import train_state\n\nimport optax\n\nfrom clu import metrics\nfrom ml_collections import ConfigDict\n\nfrom einops import reduce\nimport wandb\n\nfrom iqadatasets.datasets import *"
  },
  {
    "objectID": "00_Supervised/01b_img_cls_wandb.html#get-the-data",
    "href": "00_Supervised/01b_img_cls_wandb.html#get-the-data",
    "title": "Image Classification in JAX using TrainState (and WandB!)",
    "section": "Get the data",
    "text": "Get the data\n\nWe’ll be using MNIST from Keras.\n\n\n(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.mnist.load_data()\n\nX_train = X_train[:,:,:,None]/255.0\nX_test = X_test[:,:,:,None]/255.0\nY_train = Y_train.astype(np.int32)\nY_test = Y_test.astype(np.int32)\n\nX_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n\n((60000, 28, 28, 1), (60000,), (10000, 28, 28, 1), (10000,))\n\n\n\ndst_train = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\ndst_val = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n\n\nconfig = {\n    \"BATCH_SIZE\": 256,\n    \"EPOCHS\": 50,\n    \"LEARNING_RATE\": 3e-4,\n}\nconfig = ConfigDict(config)\nconfig\n\nBATCH_SIZE: 256\nEPOCHS: 50\nLEARNING_RATE: 0.0003\n\n\n\nwandb.init(project=\"MNIST_JAX\",\n           name=\"Single_Forward\",\n           job_type=\"training\",\n           config=config,\n           mode=\"online\",\n           )\nconfig = config\nconfig\n\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\nwandb: Currently logged in as: jorgvt. Use `wandb login --relogin` to force relogin\n\n\nTracking run with wandb version 0.15.0\n\n\nRun data is saved locally in /media/disk/users/vitojor/JaxPlayground/Notebooks/00_Supervised/wandb/run-20230426_173335-ovzphed8\n\n\nSyncing run Single_Forward to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/jorgvt/MNIST_JAX\n\n\n View run at https://wandb.ai/jorgvt/MNIST_JAX/runs/ovzphed8\n\n\nBATCH_SIZE: 256\nEPOCHS: 50\nLEARNING_RATE: 0.0003\n\n\n\ndst_train_rdy = dst_train.batch(config.BATCH_SIZE)\ndst_val_rdy = dst_val.batch(config.BATCH_SIZE)"
  },
  {
    "objectID": "00_Supervised/01b_img_cls_wandb.html#define-the-model-were-going-to-use",
    "href": "00_Supervised/01b_img_cls_wandb.html#define-the-model-were-going-to-use",
    "title": "Image Classification in JAX using TrainState (and WandB!)",
    "section": "Define the model we’re going to use",
    "text": "Define the model we’re going to use\n\nIt’s going to be a very simple model just for demonstration purposes.\n\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self,\n                 inputs,\n                 **kwargs,\n                 ):\n        outputs = nn.Conv(features=32, kernel_size=(3,3))(inputs)\n        outputs = nn.relu(outputs)\n        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n        outputs = nn.Conv(features=64, kernel_size=(3,3))(outputs)\n        outputs = nn.relu(outputs)\n        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n        outputs = reduce(outputs, \"b h w c -&gt; b c\", reduction=\"mean\")\n        outputs = nn.Dense(10)(outputs)\n        return outputs"
  },
  {
    "objectID": "00_Supervised/01b_img_cls_wandb.html#define-the-metrics-with-clu",
    "href": "00_Supervised/01b_img_cls_wandb.html#define-the-metrics-with-clu",
    "title": "Image Classification in JAX using TrainState (and WandB!)",
    "section": "Define the metrics with clu",
    "text": "Define the metrics with clu\n\n@struct.dataclass\nclass Metrics(metrics.Collection):\n    \"\"\"Collection of metrics to be tracked during training.\"\"\"\n    accuracy: metrics.Accuracy\n    loss: metrics.Average.from_output(\"loss\")\n\nBy default, TrainState doesn’t include metrics, but it’s very easy to subclass it so that it does:\n\nclass TrainState(train_state.TrainState):\n    metrics: Metrics\n\nWe’ll define a function that initializes the TrainState from a module, a rng key and some optimizer:\n\ndef create_train_state(module, key, tx, input_shape):\n    \"\"\"Creates the initial `TrainState`.\"\"\"\n    params = module.init(key, jnp.ones(input_shape))[\"params\"]\n    return TrainState.create(\n        apply_fn=module.apply,\n        params=params,\n        tx=tx,\n        metrics=Metrics.empty()\n    )"
  },
  {
    "objectID": "00_Supervised/01b_img_cls_wandb.html#defining-the-training-step",
    "href": "00_Supervised/01b_img_cls_wandb.html#defining-the-training-step",
    "title": "Image Classification in JAX using TrainState (and WandB!)",
    "section": "Defining the training step",
    "text": "Defining the training step\n\nWe want to write a function that takes the TrainState and a batch of data can performs an optimization step.\n\n\n@jax.jit\ndef train_step(state, batch):\n    \"\"\"Train for a single step.\"\"\"\n    inputs, labels = batch\n    def loss_fn(params):\n        pred = state.apply_fn({\"params\": params}, inputs)\n        loss = optax.softmax_cross_entropy_with_integer_labels(logits=pred, labels=labels).mean()\n        return loss, pred\n    (loss, pred), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n    state = state.apply_gradients(grads=grads)\n    def compute_metrics(state):\n        metric_updates = state.metrics.single_from_model_output(\n            logits=pred, labels=labels, loss=loss,\n        )\n        metrics = state.metrics.merge(metric_updates)\n        state = state.replace(metrics=metrics)\n        return state\n    state = compute_metrics(state)\n    return state\n\nIn their example, they don’t calculate the metrics at the same time. I think it is kind of a waste because it means having to perform a new forward pass, but we’ll follow as of now. Let’s define a function to perform metric calculation:\n\n@jax.jit\ndef compute_metrics(*, state, batch):\n    \"\"\"Obtaining the metrics for a given batch.\"\"\"\n    inputs, labels = batch\n    pred = state.apply_fn({\"params\": state.params}, inputs)\n    loss = optax.softmax_cross_entropy_with_integer_labels(logits=pred, labels=labels).mean()\n    metric_updates = state.metrics.single_from_model_output(\n        logits=pred, labels=labels, loss=loss,\n    )\n    metrics = state.metrics.merge(metric_updates)\n    state = state.replace(metrics=metrics)\n    return state"
  },
  {
    "objectID": "00_Supervised/01b_img_cls_wandb.html#train-the-model",
    "href": "00_Supervised/01b_img_cls_wandb.html#train-the-model",
    "title": "Image Classification in JAX using TrainState (and WandB!)",
    "section": "Train the model!",
    "text": "Train the model!\n\nstate = create_train_state(Model(), random.PRNGKey(0), optax.adam(config.LEARNING_RATE), input_shape=(1,28,28,1))\n\n\nmetrics_history = {\n    \"train_loss\": [],\n    \"val_loss\": [],\n    \"train_accuracy\": [],\n    \"val_accuracy\": [],\n}\n\n\nfor epoch in range(config.EPOCHS):\n    ## Training\n    for batch in dst_train_rdy.as_numpy_iterator():\n        state = train_step(state, batch)\n        # state = compute_metrics(state=state, batch=batch)\n        # break\n\n    ## Log the metrics\n    for name, value in state.metrics.compute().items():\n        metrics_history[f\"train_{name}\"].append(value)\n    \n    ## Empty the metrics\n    state = state.replace(metrics=state.metrics.empty())\n\n    ## Evaluation\n    for batch in dst_val_rdy.as_numpy_iterator():\n        state = compute_metrics(state=state, batch=batch)\n        # break\n    for name, value in state.metrics.compute().items():\n        metrics_history[f\"val_{name}\"].append(value)\n    state = state.replace(metrics=state.metrics.empty())\n    \n    wandb.log({\"epoch\": epoch+1, **{name:values[-1] for name, values in metrics_history.items()}})\n    print(f'Epoch {epoch} -&gt; [Train] Loss: {metrics_history[\"train_loss\"][-1]} | Accuracy: {metrics_history[\"train_accuracy\"][-1]} [Val] Loss: {metrics_history[\"val_loss\"][-1]} | Accuracy: {metrics_history[\"val_accuracy\"][-1]}')\n    # break\n\n2023-04-26 17:33:50.399662: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [60000]\n     [[{{node Placeholder/_1}}]]\n2023-04-26 17:33:53.381922: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [10000]\n     [[{{node Placeholder/_1}}]]\n\n\nEpoch 0 -&gt; [Train] Loss: 2.182008743286133 | Accuracy: 0.2583500146865845 [Val] Loss: 1.936364769935608 | Accuracy: 0.42110002040863037\nEpoch 1 -&gt; [Train] Loss: 1.6769200563430786 | Accuracy: 0.4977000057697296 [Val] Loss: 1.4756618738174438 | Accuracy: 0.5753000378608704\nEpoch 2 -&gt; [Train] Loss: 1.3775689601898193 | Accuracy: 0.593250036239624 [Val] Loss: 1.263392448425293 | Accuracy: 0.6458000540733337\nEpoch 3 -&gt; [Train] Loss: 1.2068145275115967 | Accuracy: 0.6537833213806152 [Val] Loss: 1.1156055927276611 | Accuracy: 0.6897000074386597\nEpoch 4 -&gt; [Train] Loss: 1.081493616104126 | Accuracy: 0.6955500245094299 [Val] Loss: 1.0011286735534668 | Accuracy: 0.7230000495910645\nEpoch 5 -&gt; [Train] Loss: 0.9830330014228821 | Accuracy: 0.7260167002677917 [Val] Loss: 0.909456729888916 | Accuracy: 0.751300036907196\nEpoch 6 -&gt; [Train] Loss: 0.9034460186958313 | Accuracy: 0.7495499849319458 [Val] Loss: 0.8353870511054993 | Accuracy: 0.7705000638961792\nEpoch 7 -&gt; [Train] Loss: 0.837811291217804 | Accuracy: 0.7681500315666199 [Val] Loss: 0.7737129926681519 | Accuracy: 0.7864000201225281\nEpoch 8 -&gt; [Train] Loss: 0.7825695872306824 | Accuracy: 0.7839333415031433 [Val] Loss: 0.7215777039527893 | Accuracy: 0.8014000654220581\nEpoch 9 -&gt; [Train] Loss: 0.7353348731994629 | Accuracy: 0.7968666553497314 [Val] Loss: 0.6768519878387451 | Accuracy: 0.8164000511169434\nEpoch 10 -&gt; [Train] Loss: 0.6941709518432617 | Accuracy: 0.8088499903678894 [Val] Loss: 0.6378747820854187 | Accuracy: 0.8267000317573547\nEpoch 11 -&gt; [Train] Loss: 0.6577768921852112 | Accuracy: 0.8183333277702332 [Val] Loss: 0.6033924221992493 | Accuracy: 0.835800051689148\nEpoch 12 -&gt; [Train] Loss: 0.625186562538147 | Accuracy: 0.8272500038146973 [Val] Loss: 0.572777509689331 | Accuracy: 0.8445000648498535\nEpoch 13 -&gt; [Train] Loss: 0.5957895517349243 | Accuracy: 0.8355000019073486 [Val] Loss: 0.5452829003334045 | Accuracy: 0.8528000116348267\nEpoch 14 -&gt; [Train] Loss: 0.5690935850143433 | Accuracy: 0.8436833620071411 [Val] Loss: 0.520549476146698 | Accuracy: 0.8609000444412231\nEpoch 15 -&gt; [Train] Loss: 0.5447360277175903 | Accuracy: 0.8504000306129456 [Val] Loss: 0.4981409013271332 | Accuracy: 0.8666000366210938\nEpoch 16 -&gt; [Train] Loss: 0.5224463939666748 | Accuracy: 0.8563166856765747 [Val] Loss: 0.47782716155052185 | Accuracy: 0.8708000183105469\nEpoch 17 -&gt; [Train] Loss: 0.5019102692604065 | Accuracy: 0.8618000149726868 [Val] Loss: 0.4594748020172119 | Accuracy: 0.8742000460624695\nEpoch 18 -&gt; [Train] Loss: 0.48303425312042236 | Accuracy: 0.8670666813850403 [Val] Loss: 0.4426192343235016 | Accuracy: 0.8786000609397888\nEpoch 19 -&gt; [Train] Loss: 0.4656265377998352 | Accuracy: 0.8720833659172058 [Val] Loss: 0.4271697700023651 | Accuracy: 0.8813000321388245\nEpoch 20 -&gt; [Train] Loss: 0.44959351420402527 | Accuracy: 0.8765333294868469 [Val] Loss: 0.41317644715309143 | Accuracy: 0.8841000199317932\nEpoch 21 -&gt; [Train] Loss: 0.4347400665283203 | Accuracy: 0.8805333375930786 [Val] Loss: 0.400164932012558 | Accuracy: 0.8879000544548035\nEpoch 22 -&gt; [Train] Loss: 0.4210180938243866 | Accuracy: 0.8843333721160889 [Val] Loss: 0.38827279210090637 | Accuracy: 0.8905000686645508\nEpoch 23 -&gt; [Train] Loss: 0.40825262665748596 | Accuracy: 0.8876000046730042 [Val] Loss: 0.3773591220378876 | Accuracy: 0.893500030040741\nEpoch 24 -&gt; [Train] Loss: 0.39640703797340393 | Accuracy: 0.8908666968345642 [Val] Loss: 0.3671402335166931 | Accuracy: 0.8960000276565552\nEpoch 25 -&gt; [Train] Loss: 0.3853464722633362 | Accuracy: 0.893500030040741 [Val] Loss: 0.3577060401439667 | Accuracy: 0.8972000479698181\nEpoch 26 -&gt; [Train] Loss: 0.3750644028186798 | Accuracy: 0.8962500095367432 [Val] Loss: 0.3489246666431427 | Accuracy: 0.8987000584602356\nEpoch 27 -&gt; [Train] Loss: 0.36541470885276794 | Accuracy: 0.8986333608627319 [Val] Loss: 0.3407832980155945 | Accuracy: 0.9004000425338745\nEpoch 28 -&gt; [Train] Loss: 0.3564613461494446 | Accuracy: 0.9010833501815796 [Val] Loss: 0.3331397473812103 | Accuracy: 0.902400016784668\nEpoch 29 -&gt; [Train] Loss: 0.3480415642261505 | Accuracy: 0.902916669845581 [Val] Loss: 0.3259875178337097 | Accuracy: 0.9034000635147095\nEpoch 30 -&gt; [Train] Loss: 0.34014928340911865 | Accuracy: 0.9047333598136902 [Val] Loss: 0.3192983865737915 | Accuracy: 0.9043000340461731\nEpoch 31 -&gt; [Train] Loss: 0.3327358663082123 | Accuracy: 0.906333327293396 [Val] Loss: 0.31295037269592285 | Accuracy: 0.9062000513076782\nEpoch 32 -&gt; [Train] Loss: 0.325710266828537 | Accuracy: 0.9078666567802429 [Val] Loss: 0.30704569816589355 | Accuracy: 0.9079000353813171\nEpoch 33 -&gt; [Train] Loss: 0.319106787443161 | Accuracy: 0.9093833565711975 [Val] Loss: 0.30143722891807556 | Accuracy: 0.909600019454956\nEpoch 34 -&gt; [Train] Loss: 0.3128489553928375 | Accuracy: 0.9111666679382324 [Val] Loss: 0.296077698469162 | Accuracy: 0.9110000729560852\nEpoch 35 -&gt; [Train] Loss: 0.30691418051719666 | Accuracy: 0.9125999808311462 [Val] Loss: 0.2910052239894867 | Accuracy: 0.9123000502586365\nEpoch 36 -&gt; [Train] Loss: 0.3013036549091339 | Accuracy: 0.9142000079154968 [Val] Loss: 0.28622308373451233 | Accuracy: 0.9141000509262085\nEpoch 37 -&gt; [Train] Loss: 0.2959589958190918 | Accuracy: 0.9154333472251892 [Val] Loss: 0.28169429302215576 | Accuracy: 0.9154000282287598\nEpoch 38 -&gt; [Train] Loss: 0.29088759422302246 | Accuracy: 0.9169000387191772 [Val] Loss: 0.27736854553222656 | Accuracy: 0.9163000583648682\nEpoch 39 -&gt; [Train] Loss: 0.28605058789253235 | Accuracy: 0.9180999994277954 [Val] Loss: 0.273225873708725 | Accuracy: 0.9175000190734863\nEpoch 40 -&gt; [Train] Loss: 0.2814214527606964 | Accuracy: 0.9194666743278503 [Val] Loss: 0.26928025484085083 | Accuracy: 0.9187000393867493\nEpoch 41 -&gt; [Train] Loss: 0.277006596326828 | Accuracy: 0.9207333326339722 [Val] Loss: 0.2654680609703064 | Accuracy: 0.9199000597000122\nEpoch 42 -&gt; [Train] Loss: 0.2727579176425934 | Accuracy: 0.9218500256538391 [Val] Loss: 0.2618121802806854 | Accuracy: 0.921000063419342\nEpoch 43 -&gt; [Train] Loss: 0.26871034502983093 | Accuracy: 0.9228166937828064 [Val] Loss: 0.25826191902160645 | Accuracy: 0.9226000308990479\nEpoch 44 -&gt; [Train] Loss: 0.26478293538093567 | Accuracy: 0.9237833619117737 [Val] Loss: 0.25480398535728455 | Accuracy: 0.9236000180244446\nEpoch 45 -&gt; [Train] Loss: 0.26100045442581177 | Accuracy: 0.9246333241462708 [Val] Loss: 0.2515222132205963 | Accuracy: 0.924500048160553\nEpoch 46 -&gt; [Train] Loss: 0.2573975622653961 | Accuracy: 0.9256166815757751 [Val] Loss: 0.24827086925506592 | Accuracy: 0.9252000451087952\nEpoch 47 -&gt; [Train] Loss: 0.2538735866546631 | Accuracy: 0.9266499876976013 [Val] Loss: 0.24520094692707062 | Accuracy: 0.9265000224113464\nEpoch 48 -&gt; [Train] Loss: 0.2504808008670807 | Accuracy: 0.9275833368301392 [Val] Loss: 0.24217760562896729 | Accuracy: 0.9275000691413879\nEpoch 49 -&gt; [Train] Loss: 0.24721772968769073 | Accuracy: 0.9284166693687439 [Val] Loss: 0.23932170867919922 | Accuracy: 0.9280000329017639\nCPU times: user 1min 7s, sys: 10.7 s, total: 1min 18s\nWall time: 26.1 s\n\n\n\nwandb.finish()\n\nWaiting for W&B process to finish... (success).\n\n\n\n\nRun history:\n\n\n\ntrain_accuracy\n▁▄▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇██████████████████████\n\n\ntrain_loss\n█▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nval_accuracy\n▁▃▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█████████████████████\n\n\nval_loss\n█▆▅▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\n\n\n\n\nRun summary:\n\n\n\n\n\ntrain_accuracy\n0.92842\n\n\ntrain_loss\n0.24722\n\n\nval_accuracy\n0.928\n\n\nval_loss\n0.23932\n\n\n\n\n\n\n\n View run Single_Forward at: https://wandb.ai/jorgvt/MNIST_JAX/runs/ovzphed8Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20230426_173335-ovzphed8/logs"
  },
  {
    "objectID": "00_Supervised/00_iqa.html",
    "href": "00_Supervised/00_iqa.html",
    "title": "Performing a simple IQA example with JaX",
    "section": "",
    "text": "# import os; os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nimport tensorflow as tf\ntf.config.set_visible_devices([], device_type='GPU')\n# physical_devices = tf.config.list_physical_devices('GPU')\n# try:\n#   tf.config.experimental.set_memory_growth(physical_devices[0], True)\n# except:\n#   # Invalid device or cannot modify virtual devices once initialized.\n#   pass\n\n2023-04-23 18:53:07.649737: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2023-04-23 18:53:07.701338: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-04-23 18:53:09.670200: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nimport jax\nfrom typing import Any, Callable, Sequence, Union\nfrom jax import lax, random, numpy as jnp\nfrom flax.core import freeze, unfreeze\nfrom flax import linen as nn\nimport optax\n\nfrom iqadatasets.datasets import *"
  },
  {
    "objectID": "00_Supervised/00_iqa.html#load-the-data",
    "href": "00_Supervised/00_iqa.html#load-the-data",
    "title": "Performing a simple IQA example with JaX",
    "section": "Load the data",
    "text": "Load the data\n\nWe’re going to employ iqadatasets to ease the loading of the data.\n\n\ndst_train = TID2008(\"/media/disk/databases/BBDD_video_image/Image_Quality/TID/TID2008/\")\ndst_val = TID2013(\"/media/disk/databases/BBDD_video_image/Image_Quality/TID/TID2013/\")\n\n\nimg, img_dist, mos = next(iter(dst_train.dataset))\nimg.shape, img_dist.shape, mos.shape\n\n2023-04-23 18:53:14.371382: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n(TensorShape([384, 512, 3]), TensorShape([384, 512, 3]), TensorShape([]))\n\n\n\nimg, img_dist, mos = next(iter(dst_val.dataset))\nimg.shape, img_dist.shape, mos.shape\n\n2023-04-23 18:55:37.101850: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n(TensorShape([384, 512, 3]), TensorShape([384, 512, 3]), TensorShape([]))"
  },
  {
    "objectID": "00_Supervised/00_iqa.html#define-the-model",
    "href": "00_Supervised/00_iqa.html#define-the-model",
    "title": "Performing a simple IQA example with JaX",
    "section": "Define the model",
    "text": "Define the model\n\nWe’re going to define a simple PerceptNet without GDN.\n\n\nclass PerceptNet(nn.Module):\n    \"\"\"IQA model inspired by the visual system.\"\"\"\n\n    @nn.compact\n    def __call__(self,\n                 inputs,\n                 ):\n        outputs = nn.Conv(features=3, kernel_size=(5,5), strides=1, padding=\"SAME\")(inputs)\n        outputs = nn.relu(outputs)\n        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n        outputs = nn.Conv(features=6, kernel_size=(5,5), strides=1, padding=\"SAME\")(inputs)\n        outputs = nn.relu(outputs)\n        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n        outputs = nn.Conv(features=128, kernel_size=(5,5), strides=1, padding=\"SAME\")(inputs)\n        outputs = nn.relu(outputs)\n        return outputs\n\n\nkey1, key2 = random.split(random.PRNGKey(0), 2)\ninputs = random.uniform(key2, shape=(384,512,3))\nmodel = PerceptNet()\nparams = model.init(key1, inputs)\njax.tree_util.tree_map(lambda x: x.shape, params)\n\nFrozenDict({\n    params: {\n        Conv_0: {\n            bias: (3,),\n            kernel: (5, 5, 3, 3),\n        },\n        Conv_1: {\n            bias: (6,),\n            kernel: (5, 5, 3, 6),\n        },\n        Conv_2: {\n            bias: (128,),\n            kernel: (5, 5, 3, 128),\n        },\n    },\n})"
  },
  {
    "objectID": "00_Supervised/00_iqa.html#define-the-training-loop",
    "href": "00_Supervised/00_iqa.html#define-the-training-loop",
    "title": "Performing a simple IQA example with JaX",
    "section": "Define the training loop",
    "text": "Define the training loop\n\nfrom functools import partial\n\n@partial(jax.jit, static_argnums=(0, 1))\ndef update_step(apply_fn, tx, img, img_dist, mos, opt_state, params):\n    def loss(params):\n        ## Forward pass through the model\n        img_pred = apply_fn(params, \n                            img)\n        img_dist_pred = apply_fn(params, \n                                 img_dist)\n        ## Calculate the distance\n        dist = ((img_pred - img_dist_pred)**2).sum(axis=(1,2,3))**(1/2)\n        \n        ## Calculate pearson correlation\n        y_true = dist.squeeze()\n        y_pred = mos.squeeze()\n        y_true_mean = y_true.mean()\n        y_pred_mean = y_pred.mean()\n        num = y_true-y_true_mean\n        num *= y_pred-y_pred_mean\n        num = num.sum()\n        denom = jnp.sqrt(jnp.sum((y_true-y_true_mean)**2))\n        denom *= jnp.sqrt(jnp.sum((y_pred-y_pred_mean)**2))\n        return num/denom\n    \n    l, grads = jax.value_and_grad(loss)(params)\n    updates, opt_state = tx.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n    return opt_state, params, l"
  },
  {
    "objectID": "00_Supervised/00_iqa.html#define-the-evaluation-loop",
    "href": "00_Supervised/00_iqa.html#define-the-evaluation-loop",
    "title": "Performing a simple IQA example with JaX",
    "section": "Define the evaluation loop",
    "text": "Define the evaluation loop\n\nfrom functools import partial\n\n@partial(jax.jit, static_argnums=(0))\ndef eval_step(apply_fn, img, img_dist, mos, params):\n    def loss(params):\n        ## Forward pass through the model\n        img_pred = apply_fn(params, \n                            img)\n        img_dist_pred = apply_fn(params, \n                                 img_dist)\n        ## Calculate the distance\n        dist = ((img_pred - img_dist_pred)**2).sum(axis=(1,2,3))**(1/2)\n        \n        ## Calculate pearson correlation\n        y_true = dist.squeeze()\n        y_pred = mos.squeeze()\n        y_true_mean = y_true.mean()\n        y_pred_mean = y_pred.mean()\n        num = y_true-y_true_mean\n        num *= y_pred-y_pred_mean\n        num = num.sum()\n        denom = jnp.sqrt(jnp.sum((y_true-y_true_mean)**2))\n        denom *= jnp.sqrt(jnp.sum((y_pred-y_pred_mean)**2))\n        return num/denom\n    \n    l = loss(params)\n    return l"
  },
  {
    "objectID": "00_Supervised/00_iqa.html#train-the-model",
    "href": "00_Supervised/00_iqa.html#train-the-model",
    "title": "Performing a simple IQA example with JaX",
    "section": "Train the model",
    "text": "Train the model\n\nkey1, key2 = random.split(random.PRNGKey(0), 2)\ninputs = random.uniform(key2, shape=(384,512,3))\nmodel = PerceptNet()\nparams = model.init(key1, inputs)\ntx = optax.adam(learning_rate=3e-4)\nopt_state = tx.init(params)\n\n\nEPOCHS = 5\nBATCH_SIZE = 32\nlosses, losses_val = [], []\nfor epoch in range(EPOCHS):\n    \n    ## Training\n    loss_epoch = []\n    for img, img_dist, mos in dst_train.dataset.batch(BATCH_SIZE):\n        img, img_dist, mos = img.numpy(), img_dist.numpy(), mos.numpy()\n        opt_state, params, loss = update_step(\n        model.apply, tx, img, img_dist, mos, opt_state, params)\n        loss_epoch.append(loss)\n        # break\n    losses.append(jnp.mean(jnp.array(loss_epoch)))\n    \n    ## Evaluation\n    loss_epoch_val = []\n    for img, img_dist, mos in dst_val.dataset.batch(BATCH_SIZE):\n        img, img_dist, mos = img.numpy(), img_dist.numpy(), mos.numpy()\n        loss_val = eval_step(model.apply, img, img_dist, mos, params)\n        loss_epoch_val.append(loss_val)\n        # break\n    losses_val.append(jnp.mean(jnp.array(loss_epoch_val)))\n\n    ## Logging\n    print(f\"Epoch {epoch} -&gt; Loss (Train): {losses[-1]} | Loss (Validation): {losses_val[-1]}\")\n    # break\n\n2023-04-23 19:01:10.347231: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-23 19:01:28.015166: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-23 19:03:05.010286: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-23 19:03:24.610020: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-23 19:03:56.007672: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-23 19:04:13.087976: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-23 19:04:40.923255: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-23 19:04:57.161878: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-23 19:05:25.436143: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-23 19:05:41.914958: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\nEpoch 0 -&gt; Loss (Train): -0.557887613773346 | Loss (Validation): -0.7108475565910339\nEpoch 1 -&gt; Loss (Train): -0.6508658528327942 | Loss (Validation): -0.7451121211051941\nEpoch 2 -&gt; Loss (Train): -0.6950182914733887 | Loss (Validation): -0.76376873254776\nEpoch 3 -&gt; Loss (Train): -0.7255797386169434 | Loss (Validation): -0.7783637642860413\nEpoch 4 -&gt; Loss (Train): -0.7484660744667053 | Loss (Validation): -0.7901502251625061\nCPU times: user 13min 23s, sys: 19min 18s, total: 32min 42s\nWall time: 4min 59s"
  },
  {
    "objectID": "00_Supervised/00b_iqa_parametric.html",
    "href": "00_Supervised/00b_iqa_parametric.html",
    "title": "IQA tracking params and variables",
    "section": "",
    "text": "# import os; os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\".99\"\nimport tensorflow as tf\ntf.config.set_visible_devices([], device_type='GPU')\n\n2023-04-30 19:56:55.578859: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2023-04-30 19:56:55.631274: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-04-30 19:56:57.698465: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nfrom typing import Any, Callable, Sequence, Union\nimport numpy as np\n\nimport jax\nfrom jax import lax, random, numpy as jnp\nfrom flax.core import freeze, unfreeze, FrozenDict\nfrom flax import linen as nn\nfrom flax import struct\nfrom flax.training import train_state\n\nimport optax\n\nfrom clu import metrics\nfrom ml_collections import ConfigDict\n\nfrom einops import reduce\n\nfrom iqadatasets.datasets import *\nfrom fxlayers.layers import *"
  },
  {
    "objectID": "00_Supervised/00b_iqa_parametric.html#load-the-data",
    "href": "00_Supervised/00b_iqa_parametric.html#load-the-data",
    "title": "IQA tracking params and variables",
    "section": "Load the data",
    "text": "Load the data\n\nWe’re going to employ iqadatasets to ease the loading of the data.\n\n\ndst_train = TID2008(\"/media/disk/databases/BBDD_video_image/Image_Quality/TID/TID2008/\")\ndst_val = TID2013(\"/media/disk/databases/BBDD_video_image/Image_Quality/TID/TID2013/\")\n\n\nimg, img_dist, mos = next(iter(dst_train.dataset))\nimg.shape, img_dist.shape, mos.shape\n\n2023-04-30 19:57:03.136692: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n(TensorShape([384, 512, 3]), TensorShape([384, 512, 3]), TensorShape([]))\n\n\n\nimg, img_dist, mos = next(iter(dst_val.dataset))\nimg.shape, img_dist.shape, mos.shape\n\n2023-04-30 19:57:03.389577: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n(TensorShape([384, 512, 3]), TensorShape([384, 512, 3]), TensorShape([]))\n\n\n\nconfig = {\n    \"BATCH_SIZE\": 32,\n    \"EPOCHS\": 5,\n    \"LEARNING_RATE\": 3e-4,\n}\nconfig = ConfigDict(config)\nconfig\n\nBATCH_SIZE: 32\nEPOCHS: 5\nLEARNING_RATE: 0.0003\n\n\n\ndst_train_rdy = dst_train.dataset.batch(config.BATCH_SIZE, drop_remainder=True)\ndst_val_rdy = dst_val.dataset.batch(config.BATCH_SIZE, drop_remainder=True)"
  },
  {
    "objectID": "00_Supervised/00b_iqa_parametric.html#define-the-model-were-going-to-use",
    "href": "00_Supervised/00b_iqa_parametric.html#define-the-model-were-going-to-use",
    "title": "IQA tracking params and variables",
    "section": "Define the model we’re going to use",
    "text": "Define the model we’re going to use\n\nIt’s going to be a very simple model just for demonstration purposes.\n\n\nclass GDN(nn.Module):\n    \"\"\"Generalized Divisive Normalization.\"\"\"\n    kernel_size: Union[int, Sequence[int]]\n    strides: int = 1\n    padding: str = \"SAME\"\n    apply_independently: bool = False\n    # feature_group_count: int = 1\n    kernel_init: Callable = nn.initializers.lecun_normal()\n    bias_init: Callable = nn.initializers.ones_init()\n    alpha: float = 2.\n    epsilon: float = 1/2 # Exponential of the denominator\n    eps: float = 1e-6 # Numerical stability in the denominator\n\n    @nn.compact\n    def __call__(self,\n                 inputs,\n                 ):\n        denom = nn.Conv(features=inputs.shape[-1], # Same output channels as input\n                        kernel_size=self.kernel_size if isinstance(self.kernel_size, Sequence) else [self.kernel_size]*2, \n                        strides=self.strides, \n                        padding=self.padding,\n                        # feature_group_count=self.feature_group_count,\n                        feature_group_count=inputs.shape[-1] if self.apply_independently else 1,\n                        kernel_init=self.kernel_init, \n                        bias_init=self.bias_init)(inputs**self.alpha)\n        denom = nn.relu(denom)\n        return inputs / (denom**self.epsilon + self.eps)\n\n\nclass PerceptNet(nn.Module):\n    \"\"\"IQA model inspired by the visual system.\"\"\"\n\n    @nn.compact\n    def __call__(self,\n                 inputs,\n                 **kwargs,\n                 ):\n        outputs = nn.Conv(features=3, kernel_size=(5,5), strides=1, padding=\"SAME\")(inputs)\n        outputs = GDN(kernel_size=1, strides=1, padding=\"SAME\", apply_independently=True)(outputs)\n        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n        outputs = nn.Conv(features=6, kernel_size=(5,5), strides=1, padding=\"SAME\")(outputs)\n        outputs = GDN(kernel_size=1, strides=1, padding=\"SAME\", apply_independently=False)(outputs)\n        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n        outputs = GaborLayer(features=128, kernel_size=21, strides=1, padding=\"SAME\", fs=21)(outputs, **kwargs)\n        # outputs = nn.Conv(features=128, kernel_size=(5,5), strides=1, padding=\"SAME\")(outputs)\n        outputs = GDN(kernel_size=1, strides=1, padding=\"SAME\", apply_independently=False)(outputs)\n        return outputs"
  },
  {
    "objectID": "00_Supervised/00b_iqa_parametric.html#define-the-metrics-with-clu",
    "href": "00_Supervised/00b_iqa_parametric.html#define-the-metrics-with-clu",
    "title": "IQA tracking params and variables",
    "section": "Define the metrics with clu",
    "text": "Define the metrics with clu\n\n@struct.dataclass\nclass Metrics(metrics.Collection):\n    \"\"\"Collection of metrics to be tracked during training.\"\"\"\n    loss: metrics.Average.from_output(\"loss\")\n\nBy default, TrainState doesn’t include metrics, but it’s very easy to subclass it so that it does:\n\nclass TrainState(train_state.TrainState):\n    metrics: Metrics\n    state: FrozenDict\n\nWe’ll define a function that initializes the TrainState from a module, a rng key and some optimizer:\n\ndef create_train_state(module, key, tx, input_shape):\n    \"\"\"Creates the initial `TrainState`.\"\"\"\n    variables = module.init(key, jnp.ones(input_shape))\n    state, params = variables.pop('params')\n    return TrainState.create(\n        apply_fn=module.apply,\n        params=params,\n        state=state,\n        tx=tx,\n        metrics=Metrics.empty()\n    )"
  },
  {
    "objectID": "00_Supervised/00b_iqa_parametric.html#defining-the-training-step",
    "href": "00_Supervised/00b_iqa_parametric.html#defining-the-training-step",
    "title": "IQA tracking params and variables",
    "section": "Defining the training step",
    "text": "Defining the training step\n\nWe want to write a function that takes the TrainState and a batch of data can performs an optimization step.\n\n\ndef pearson_correlation(vec1, vec2):\n    vec1 = vec1.squeeze()\n    vec2 = vec2.squeeze()\n    vec1_mean = vec1.mean()\n    vec2_mean = vec2.mean()\n    num = vec1-vec1_mean\n    num *= vec2-vec2_mean\n    num = num.sum()\n    denom = jnp.sqrt(jnp.sum((vec1-vec1_mean)**2))\n    denom *= jnp.sqrt(jnp.sum((vec2-vec2_mean)**2))\n    return num/denom\n\n\n@jax.jit\ndef train_step(state, batch):\n    \"\"\"Train for a single step.\"\"\"\n    img, img_dist, mos = batch\n    def loss_fn(params):\n        ## Forward pass through the model\n        img_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img, mutable=list(state.state.keys()), train=True)\n        img_dist_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img_dist, mutable=list(state.state.keys()), train=True)\n\n        ## Calculate the distance\n        dist = ((img_pred - img_dist_pred)**2).sum(axis=(1,2,3))**(1/2)\n        \n        ## Calculate pearson correlation\n        return pearson_correlation(dist, mos), updated_state\n    \n    (loss, updated_state), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n    state = state.apply_gradients(grads=grads)\n    metrics_updates = state.metrics.single_from_model_output(loss=loss)\n    metrics = state.metrics.merge(metrics_updates)\n    state = state.replace(metrics=metrics)\n    state = state.replace(state=updated_state)\n    return state\n\nIn their example, they don’t calculate the metrics at the same time. I think it is kind of a waste because it means having to perform a new forward pass, but we’ll follow as of now. Let’s define a function to perform metric calculation:\n\n@jax.jit\ndef compute_metrics(*, state, batch):\n    \"\"\"Obtaining the metrics for a given batch.\"\"\"\n    img, img_dist, mos = batch\n    def loss_fn(params):\n        ## Forward pass through the model\n        img_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img, mutable=list(state.state.keys()), train=True)\n        img_dist_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img_dist, mutable=list(state.state.keys()), train=True)\n\n        ## Calculate the distance\n        dist = ((img_pred - img_dist_pred)**2).sum(axis=(1,2,3))**(1/2)\n        \n        ## Calculate pearson correlation\n        return pearson_correlation(dist, mos)\n    \n    metrics_updates = state.metrics.single_from_model_output(loss=loss_fn(state.params))\n    metrics = state.metrics.merge(metrics_updates)\n    state = state.replace(metrics=metrics)\n    return state"
  },
  {
    "objectID": "00_Supervised/00b_iqa_parametric.html#train-the-model",
    "href": "00_Supervised/00b_iqa_parametric.html#train-the-model",
    "title": "IQA tracking params and variables",
    "section": "Train the model!",
    "text": "Train the model!\n\nstate = create_train_state(PerceptNet(), random.PRNGKey(0), optax.adam(config.LEARNING_RATE), input_shape=(1,384,512,3))\n\n\nmetrics_history = {\n    \"train_loss\": [],\n    \"val_loss\": [],\n}\n\n\nfor epoch in range(config.EPOCHS):\n    ## Training\n    for batch in dst_train_rdy.as_numpy_iterator():\n        state = train_step(state, batch)\n        # state = compute_metrics(state=state, batch=batch)\n        # break\n\n    ## Log the metrics\n    for name, value in state.metrics.compute().items():\n        metrics_history[f\"train_{name}\"].append(value)\n    \n    ## Empty the metrics\n    state = state.replace(metrics=state.metrics.empty())\n\n    ## Evaluation\n    for batch in dst_val_rdy.as_numpy_iterator():\n        state = compute_metrics(state=state, batch=batch)\n        # break\n    for name, value in state.metrics.compute().items():\n        metrics_history[f\"val_{name}\"].append(value)\n    state = state.replace(metrics=state.metrics.empty())\n    \n    print(f'Epoch {epoch} -&gt; [Train] Loss: {metrics_history[\"train_loss\"][-1]} [Val] Loss: {metrics_history[\"val_loss\"][-1]}')\n    # break\n\n2023-04-30 19:57:15.232964: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-30 19:58:01.771566: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\nEpoch 0 -&gt; [Train] Loss: -0.3694571852684021 [Val] Loss: -0.5647238492965698\nEpoch 1 -&gt; [Train] Loss: -0.4500204622745514 [Val] Loss: -0.6486803889274597\nEpoch 2 -&gt; [Train] Loss: -0.5101209878921509 [Val] Loss: -0.5459914207458496\nEpoch 3 -&gt; [Train] Loss: -0.4495947062969208 [Val] Loss: -0.6351958513259888\nEpoch 4 -&gt; [Train] Loss: -0.32780590653419495 [Val] Loss: -0.38518646359443665\nCPU times: user 13min 26s, sys: 18min 59s, total: 32min 26s\nWall time: 4min 21s\n\n\n\nimport matplotlib.pyplot as plt\nfrom einops import rearrange\n\n\nfilters = rearrange(state.state[\"precalc_filter\"][\"GaborLayer_0\"][\"kernel\"], \"kx ky cin cout -&gt; (cin cout) kx ky\")[:10]\nfig, axes = plt.subplots(1,len(filters))\nfor k, ax in zip(filters, axes.ravel()):\n    ax.imshow(k)\n    ax.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "00_Supervised/01c_checkpointing.html",
    "href": "00_Supervised/01c_checkpointing.html",
    "title": "Image Classification in JAX using TrainState (and WandB!) ((and checkpointing!!))",
    "section": "",
    "text": "import tensorflow as tf\ntf.config.set_visible_devices([], device_type='GPU')\n\n2023-04-27 12:53:59.255932: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2023-04-27 12:53:59.310993: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-04-27 12:54:01.377224: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nimport os\nfrom typing import Any, Callable, Sequence, Union\nimport numpy as np\n\nimport jax\nfrom jax import lax, random, numpy as jnp\nfrom flax.core import freeze, unfreeze\nfrom flax import linen as nn\nfrom flax import struct\nfrom flax.training import train_state\nfrom flax.training import orbax_utils\n\nimport optax\nimport orbax.checkpoint\n\nfrom clu import metrics\nfrom ml_collections import ConfigDict\n\nfrom einops import reduce\nimport wandb\n\nfrom iqadatasets.datasets import *"
  },
  {
    "objectID": "00_Supervised/01c_checkpointing.html#get-the-data",
    "href": "00_Supervised/01c_checkpointing.html#get-the-data",
    "title": "Image Classification in JAX using TrainState (and WandB!) ((and checkpointing!!))",
    "section": "Get the data",
    "text": "Get the data\n\nWe’ll be using MNIST from Keras.\n\n\n(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.mnist.load_data()\n\nX_train = X_train[:,:,:,None]/255.0\nX_test = X_test[:,:,:,None]/255.0\nY_train = Y_train.astype(np.int32)\nY_test = Y_test.astype(np.int32)\n\nX_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n\n((60000, 28, 28, 1), (60000,), (10000, 28, 28, 1), (10000,))\n\n\n\ndst_train = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\ndst_val = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n\n\nconfig = {\n    \"BATCH_SIZE\": 256,\n    \"EPOCHS\": 50,\n    \"LEARNING_RATE\": 3e-4,\n}\nconfig = ConfigDict(config)\nconfig\n\nBATCH_SIZE: 256\nEPOCHS: 50\nLEARNING_RATE: 0.0003\n\n\n\nwandb.init(project=\"MNIST_JAX\",\n           name=\"Single_Forward\",\n           job_type=\"training\",\n           config=config,\n           mode=\"online\",\n           )\nconfig = config\nconfig\n\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\nwandb: Currently logged in as: jorgvt. Use `wandb login --relogin` to force relogin\n\n\nTracking run with wandb version 0.15.0\n\n\nRun data is saved locally in /media/disk/users/vitojor/JaxPlayground/Notebooks/00_Supervised/wandb/run-20230427_125410-u9x7wdmf\n\n\nSyncing run Single_Forward to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/jorgvt/MNIST_JAX\n\n\n View run at https://wandb.ai/jorgvt/MNIST_JAX/runs/u9x7wdmf\n\n\nBATCH_SIZE: 256\nEPOCHS: 50\nLEARNING_RATE: 0.0003\n\n\n\ndst_train_rdy = dst_train.batch(config.BATCH_SIZE)\ndst_val_rdy = dst_val.batch(config.BATCH_SIZE)"
  },
  {
    "objectID": "00_Supervised/01c_checkpointing.html#define-the-model-were-going-to-use",
    "href": "00_Supervised/01c_checkpointing.html#define-the-model-were-going-to-use",
    "title": "Image Classification in JAX using TrainState (and WandB!) ((and checkpointing!!))",
    "section": "Define the model we’re going to use",
    "text": "Define the model we’re going to use\n\nIt’s going to be a very simple model just for demonstration purposes.\n\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self,\n                 inputs,\n                 **kwargs,\n                 ):\n        outputs = nn.Conv(features=32, kernel_size=(3,3))(inputs)\n        outputs = nn.relu(outputs)\n        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n        outputs = nn.Conv(features=64, kernel_size=(3,3))(outputs)\n        outputs = nn.relu(outputs)\n        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n        outputs = reduce(outputs, \"b h w c -&gt; b c\", reduction=\"mean\")\n        outputs = nn.Dense(10)(outputs)\n        return outputs"
  },
  {
    "objectID": "00_Supervised/01c_checkpointing.html#define-the-metrics-with-clu",
    "href": "00_Supervised/01c_checkpointing.html#define-the-metrics-with-clu",
    "title": "Image Classification in JAX using TrainState (and WandB!) ((and checkpointing!!))",
    "section": "Define the metrics with clu",
    "text": "Define the metrics with clu\n\n@struct.dataclass\nclass Metrics(metrics.Collection):\n    \"\"\"Collection of metrics to be tracked during training.\"\"\"\n    accuracy: metrics.Accuracy\n    loss: metrics.Average.from_output(\"loss\")\n\nBy default, TrainState doesn’t include metrics, but it’s very easy to subclass it so that it does:\n\nclass TrainState(train_state.TrainState):\n    metrics: Metrics\n\nWe’ll define a function that initializes the TrainState from a module, a rng key and some optimizer:\n\ndef create_train_state(module, key, tx, input_shape):\n    \"\"\"Creates the initial `TrainState`.\"\"\"\n    params = module.init(key, jnp.ones(input_shape))[\"params\"]\n    return TrainState.create(\n        apply_fn=module.apply,\n        params=params,\n        tx=tx,\n        metrics=Metrics.empty()\n    )"
  },
  {
    "objectID": "00_Supervised/01c_checkpointing.html#defining-the-training-step",
    "href": "00_Supervised/01c_checkpointing.html#defining-the-training-step",
    "title": "Image Classification in JAX using TrainState (and WandB!) ((and checkpointing!!))",
    "section": "Defining the training step",
    "text": "Defining the training step\n\nWe want to write a function that takes the TrainState and a batch of data can performs an optimization step.\n\n\n@jax.jit\ndef train_step(state, batch):\n    \"\"\"Train for a single step.\"\"\"\n    inputs, labels = batch\n    def loss_fn(params):\n        pred = state.apply_fn({\"params\": params}, inputs)\n        loss = optax.softmax_cross_entropy_with_integer_labels(logits=pred, labels=labels).mean()\n        return loss, pred\n    (loss, pred), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n    state = state.apply_gradients(grads=grads)\n    def compute_metrics(state):\n        metric_updates = state.metrics.single_from_model_output(\n            logits=pred, labels=labels, loss=loss,\n        )\n        metrics = state.metrics.merge(metric_updates)\n        state = state.replace(metrics=metrics)\n        return state\n    state = compute_metrics(state)\n    return state\n\nIn their example, they don’t calculate the metrics at the same time. I think it is kind of a waste because it means having to perform a new forward pass, but we’ll follow as of now. Let’s define a function to perform metric calculation:\n\n@jax.jit\ndef compute_metrics(*, state, batch):\n    \"\"\"Obtaining the metrics for a given batch.\"\"\"\n    inputs, labels = batch\n    pred = state.apply_fn({\"params\": state.params}, inputs)\n    loss = optax.softmax_cross_entropy_with_integer_labels(logits=pred, labels=labels).mean()\n    metric_updates = state.metrics.single_from_model_output(\n        logits=pred, labels=labels, loss=loss,\n    )\n    metrics = state.metrics.merge(metric_updates)\n    state = state.replace(metrics=metrics)\n    return state"
  },
  {
    "objectID": "00_Supervised/01c_checkpointing.html#train-the-model",
    "href": "00_Supervised/01c_checkpointing.html#train-the-model",
    "title": "Image Classification in JAX using TrainState (and WandB!) ((and checkpointing!!))",
    "section": "Train the model!",
    "text": "Train the model!\n\nstate = create_train_state(Model(), random.PRNGKey(0), optax.adam(config.LEARNING_RATE), input_shape=(1,28,28,1))\n\nBefore actually training the model we’re going to set up the checkpointer to be able to save our trained models:\n\norbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\nsave_args = orbax_utils.save_args_from_target(state)\n# orbax_checkpointer.save(\"test_save\", state, save_args=save_args)\n\nTo be able to use versioning and automatic bookkeeping we need to wrap PyTreeCheckpointer with orbax.checkpoint.CheckpointManager. This allows us to customize the saving even more if we need to. As saving a model is an I/O operation, we may benefit from doing it asyncronously. This is as easy as using AsyncCheckpointer instead of PyTreeCheckpointer.\n\nmetrics_history = {\n    \"train_loss\": [],\n    \"val_loss\": [],\n    \"train_accuracy\": [],\n    \"val_accuracy\": [],\n}\n\n\nfor epoch in range(config.EPOCHS):\n    ## Training\n    for batch in dst_train_rdy.as_numpy_iterator():\n        state = train_step(state, batch)\n        # state = compute_metrics(state=state, batch=batch)\n        # break\n\n    ## Log the metrics\n    for name, value in state.metrics.compute().items():\n        metrics_history[f\"train_{name}\"].append(value)\n    \n    ## Empty the metrics\n    state = state.replace(metrics=state.metrics.empty())\n\n    ## Evaluation\n    for batch in dst_val_rdy.as_numpy_iterator():\n        state = compute_metrics(state=state, batch=batch)\n        # break\n    for name, value in state.metrics.compute().items():\n        metrics_history[f\"val_{name}\"].append(value)\n    state = state.replace(metrics=state.metrics.empty())\n\n    ## Checkpointing\n    if metrics_history[\"val_accuracy\"][-1] &gt;= max(metrics_history[\"val_accuracy\"]):\n        orbax_checkpointer.save(os.path.join(wandb.run.dir, \"model\"), state, save_args=save_args, force=True) # force=True means allow overwritting.\n\n    \n    wandb.log({\"epoch\": epoch+1, **{name:values[-1] for name, values in metrics_history.items()}})\n    print(f'Epoch {epoch} -&gt; [Train] Loss: {metrics_history[\"train_loss\"][-1]} | Accuracy: {metrics_history[\"train_accuracy\"][-1]} [Val] Loss: {metrics_history[\"val_loss\"][-1]} | Accuracy: {metrics_history[\"val_accuracy\"][-1]}')\n    # break\n\n2023-04-27 12:54:25.313858: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [60000]\n     [[{{node Placeholder/_1}}]]\n2023-04-27 12:54:28.345723: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [10000]\n     [[{{node Placeholder/_1}}]]\n\n\nEpoch 0 -&gt; [Train] Loss: 2.1820199489593506 | Accuracy: 0.2582666575908661 [Val] Loss: 1.9363574981689453 | Accuracy: 0.42100003361701965\nEpoch 1 -&gt; [Train] Loss: 1.67693293094635 | Accuracy: 0.4978833496570587 [Val] Loss: 1.4757040739059448 | Accuracy: 0.5752000212669373\nEpoch 2 -&gt; [Train] Loss: 1.377483606338501 | Accuracy: 0.5930666923522949 [Val] Loss: 1.2632259130477905 | Accuracy: 0.6459000110626221\nEpoch 3 -&gt; [Train] Loss: 1.2066980600357056 | Accuracy: 0.6537666916847229 [Val] Loss: 1.1153720617294312 | Accuracy: 0.6894000172615051\nEpoch 4 -&gt; [Train] Loss: 1.081371545791626 | Accuracy: 0.6956666707992554 [Val] Loss: 1.0010062456130981 | Accuracy: 0.7227000594139099\nEpoch 5 -&gt; [Train] Loss: 0.9829501509666443 | Accuracy: 0.7260167002677917 [Val] Loss: 0.9094106554985046 | Accuracy: 0.7510000467300415\nEpoch 6 -&gt; [Train] Loss: 0.9034146666526794 | Accuracy: 0.7497000098228455 [Val] Loss: 0.8354026079177856 | Accuracy: 0.770300030708313\nEpoch 7 -&gt; [Train] Loss: 0.8378530740737915 | Accuracy: 0.7681166529655457 [Val] Loss: 0.7736756801605225 | Accuracy: 0.7862000465393066\nEpoch 8 -&gt; [Train] Loss: 0.782595157623291 | Accuracy: 0.7839333415031433 [Val] Loss: 0.721613347530365 | Accuracy: 0.801300048828125\nEpoch 9 -&gt; [Train] Loss: 0.7353770136833191 | Accuracy: 0.7968166470527649 [Val] Loss: 0.6770400404930115 | Accuracy: 0.8156000375747681\nEpoch 10 -&gt; [Train] Loss: 0.6942520141601562 | Accuracy: 0.8086666464805603 [Val] Loss: 0.6380061507225037 | Accuracy: 0.8267000317573547\nEpoch 11 -&gt; [Train] Loss: 0.6578672528266907 | Accuracy: 0.8182333707809448 [Val] Loss: 0.6035601496696472 | Accuracy: 0.8357000350952148\nEpoch 12 -&gt; [Train] Loss: 0.6253001689910889 | Accuracy: 0.8272666931152344 [Val] Loss: 0.5729242563247681 | Accuracy: 0.844700038433075\nEpoch 13 -&gt; [Train] Loss: 0.5958832502365112 | Accuracy: 0.8356500267982483 [Val] Loss: 0.5454649329185486 | Accuracy: 0.8524000644683838\nEpoch 14 -&gt; [Train] Loss: 0.569243848323822 | Accuracy: 0.8437333703041077 [Val] Loss: 0.5207400321960449 | Accuracy: 0.8609000444412231\nEpoch 15 -&gt; [Train] Loss: 0.5449531674385071 | Accuracy: 0.8503666520118713 [Val] Loss: 0.49839162826538086 | Accuracy: 0.8663000464439392\nEpoch 16 -&gt; [Train] Loss: 0.5226361155509949 | Accuracy: 0.8562333583831787 [Val] Loss: 0.4781108796596527 | Accuracy: 0.8708000183105469\nEpoch 17 -&gt; [Train] Loss: 0.5021227598190308 | Accuracy: 0.8616333603858948 [Val] Loss: 0.45969006419181824 | Accuracy: 0.8740000128746033\nEpoch 18 -&gt; [Train] Loss: 0.4832536578178406 | Accuracy: 0.8670333623886108 [Val] Loss: 0.4427900016307831 | Accuracy: 0.8785000443458557\nEpoch 19 -&gt; [Train] Loss: 0.4658797085285187 | Accuracy: 0.8720333576202393 [Val] Loss: 0.42743808031082153 | Accuracy: 0.8810000419616699\nEpoch 20 -&gt; [Train] Loss: 0.44983699917793274 | Accuracy: 0.8764833211898804 [Val] Loss: 0.4134153425693512 | Accuracy: 0.8843000531196594\nEpoch 21 -&gt; [Train] Loss: 0.43500810861587524 | Accuracy: 0.8805000185966492 [Val] Loss: 0.4005475640296936 | Accuracy: 0.8878000378608704\nEpoch 22 -&gt; [Train] Loss: 0.4212695360183716 | Accuracy: 0.8841333389282227 [Val] Loss: 0.38852593302726746 | Accuracy: 0.8904000520706177\nEpoch 23 -&gt; [Train] Loss: 0.4085208475589752 | Accuracy: 0.887583315372467 [Val] Loss: 0.37755367159843445 | Accuracy: 0.8936000466346741\nEpoch 24 -&gt; [Train] Loss: 0.3966846764087677 | Accuracy: 0.8907666802406311 [Val] Loss: 0.3673703372478485 | Accuracy: 0.8962000608444214\nEpoch 25 -&gt; [Train] Loss: 0.38564103841781616 | Accuracy: 0.8933833241462708 [Val] Loss: 0.3579704165458679 | Accuracy: 0.8970000147819519\nEpoch 26 -&gt; [Train] Loss: 0.3753395974636078 | Accuracy: 0.8961166739463806 [Val] Loss: 0.34921061992645264 | Accuracy: 0.8986000418663025\nEpoch 27 -&gt; [Train] Loss: 0.3657287061214447 | Accuracy: 0.8985333442687988 [Val] Loss: 0.3410508334636688 | Accuracy: 0.9005000591278076\nEpoch 28 -&gt; [Train] Loss: 0.35672497749328613 | Accuracy: 0.9010500311851501 [Val] Loss: 0.3333248198032379 | Accuracy: 0.9026000499725342\nEpoch 29 -&gt; [Train] Loss: 0.34829211235046387 | Accuracy: 0.9030166864395142 [Val] Loss: 0.3262018859386444 | Accuracy: 0.9036000370979309\nEpoch 30 -&gt; [Train] Loss: 0.34040912985801697 | Accuracy: 0.9047999978065491 [Val] Loss: 0.3194579780101776 | Accuracy: 0.9045000672340393\nEpoch 31 -&gt; [Train] Loss: 0.33296310901641846 | Accuracy: 0.9061999917030334 [Val] Loss: 0.3132016062736511 | Accuracy: 0.9064000248908997\nEpoch 32 -&gt; [Train] Loss: 0.32594195008277893 | Accuracy: 0.9077666997909546 [Val] Loss: 0.3072311282157898 | Accuracy: 0.9080000519752502\nEpoch 33 -&gt; [Train] Loss: 0.319314181804657 | Accuracy: 0.9093000292778015 [Val] Loss: 0.30153241753578186 | Accuracy: 0.9094000458717346\nEpoch 34 -&gt; [Train] Loss: 0.31306859850883484 | Accuracy: 0.9110000133514404 [Val] Loss: 0.2962201237678528 | Accuracy: 0.9110000729560852\nEpoch 35 -&gt; [Train] Loss: 0.3071337640285492 | Accuracy: 0.9126666784286499 [Val] Loss: 0.29112088680267334 | Accuracy: 0.9123000502586365\nEpoch 36 -&gt; [Train] Loss: 0.3015291392803192 | Accuracy: 0.9140499830245972 [Val] Loss: 0.28640487790107727 | Accuracy: 0.9139000177383423\nEpoch 37 -&gt; [Train] Loss: 0.2961737811565399 | Accuracy: 0.9154000282287598 [Val] Loss: 0.28188082575798035 | Accuracy: 0.9153000712394714\nEpoch 38 -&gt; [Train] Loss: 0.29110246896743774 | Accuracy: 0.9168500304222107 [Val] Loss: 0.2775653302669525 | Accuracy: 0.916100025177002\nEpoch 39 -&gt; [Train] Loss: 0.2862601578235626 | Accuracy: 0.9182167053222656 [Val] Loss: 0.2733447253704071 | Accuracy: 0.917400062084198\nEpoch 40 -&gt; [Train] Loss: 0.28162434697151184 | Accuracy: 0.9193000197410583 [Val] Loss: 0.2694474756717682 | Accuracy: 0.9187000393867493\nEpoch 41 -&gt; [Train] Loss: 0.2772080898284912 | Accuracy: 0.9207000136375427 [Val] Loss: 0.2655843198299408 | Accuracy: 0.9201000332832336\nEpoch 42 -&gt; [Train] Loss: 0.2729730010032654 | Accuracy: 0.9217666983604431 [Val] Loss: 0.261908620595932 | Accuracy: 0.9214000701904297\nEpoch 43 -&gt; [Train] Loss: 0.2688678205013275 | Accuracy: 0.9227833151817322 [Val] Loss: 0.2584514319896698 | Accuracy: 0.9222000241279602\nEpoch 44 -&gt; [Train] Loss: 0.2649327516555786 | Accuracy: 0.92371666431427 [Val] Loss: 0.2549607455730438 | Accuracy: 0.9235000610351562\nEpoch 45 -&gt; [Train] Loss: 0.26115134358406067 | Accuracy: 0.9246500134468079 [Val] Loss: 0.2516634464263916 | Accuracy: 0.9244000315666199\nEpoch 46 -&gt; [Train] Loss: 0.2575255036354065 | Accuracy: 0.9256666898727417 [Val] Loss: 0.24841749668121338 | Accuracy: 0.9253000617027283\nEpoch 47 -&gt; [Train] Loss: 0.2540118396282196 | Accuracy: 0.9266166687011719 [Val] Loss: 0.24537406861782074 | Accuracy: 0.926300048828125\nEpoch 48 -&gt; [Train] Loss: 0.25061294436454773 | Accuracy: 0.9276166558265686 [Val] Loss: 0.24225929379463196 | Accuracy: 0.9277000427246094\nEpoch 49 -&gt; [Train] Loss: 0.24733121693134308 | Accuracy: 0.928350031375885 [Val] Loss: 0.23935328423976898 | Accuracy: 0.9284000396728516\nCPU times: user 1min 14s, sys: 12.8 s, total: 1min 27s\nWall time: 30.5 s\n\n\n\nwandb.finish()\n\nWaiting for W&B process to finish... (success).\n\n\n\n\nRun history:\n\n\n\nepoch\n▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n\n\ntrain_accuracy\n▁▄▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇███████████████████████\n\n\ntrain_loss\n█▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nval_accuracy\n▁▃▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█████████████████████\n\n\nval_loss\n█▆▅▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\n\n\n\n\nRun summary:\n\n\n\n\n\nepoch\n50\n\n\ntrain_accuracy\n0.92835\n\n\ntrain_loss\n0.24733\n\n\nval_accuracy\n0.9284\n\n\nval_loss\n0.23935\n\n\n\n\n\n\n\n View run Single_Forward at: https://wandb.ai/jorgvt/MNIST_JAX/runs/u9x7wdmfSynced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n\n\nFind logs at: ./wandb/run-20230427_125410-u9x7wdmf/logs"
  },
  {
    "objectID": "00_Supervised/01c_checkpointing.html#restore-the-trained-model",
    "href": "00_Supervised/01c_checkpointing.html#restore-the-trained-model",
    "title": "Image Classification in JAX using TrainState (and WandB!) ((and checkpointing!!))",
    "section": "Restore the trained model",
    "text": "Restore the trained model\n\nWe have trained our model, let’s see if we can load the trained weights.\n\n\nnew_state = create_train_state(Model(), random.PRNGKey(0), optax.adam(config.LEARNING_RATE), input_shape=(1,28,28,1))\nassert not new_state == state\n\nTo restore a saved checkpoint we only have to call the .restore() method of the checkpointer:\n\nnew_state = orbax_checkpointer.restore(\"test_save\")\n\n\nnew_state[\"opt_state\"][0][\"count\"] == state.opt_state[0].count\n\nArray(True, dtype=bool)\n\n\nWe were able to load the same state but it was loaded as a normal Python dict, not as a TrainState. If we want to load it as a custom object we have to probide Orbax a example of the type of PyTree that we want to load. First we’ll reinstantiate a new TrainState and then we will pass it to .restore(item=sample_object) with the item argument:\n\nnew_state = create_train_state(Model(), random.PRNGKey(0), optax.adam(config.LEARNING_RATE), input_shape=(1,28,28,1))\nassert not new_state == state\n\n\nnew_state = orbax_checkpointer.restore(\"test_save\", item=new_state)\nassert not new_state == state\n\nIf we test for equality we will get a False result, but that is because the restoration loads the original jnp.Array as np.array, but their content will be the same.\n\njax.tree_util.tree_map(lambda x,y: (x==y).all(), state.params, new_state.params)\n\nFrozenDict({\n    Conv_0: {\n        bias: Array(True, dtype=bool),\n        kernel: Array(True, dtype=bool),\n    },\n    Conv_1: {\n        bias: Array(True, dtype=bool),\n        kernel: Array(True, dtype=bool),\n    },\n    Dense_0: {\n        bias: Array(True, dtype=bool),\n        kernel: Array(True, dtype=bool),\n    },\n})\n\n\n\njax.tree_util.tree_map(lambda x,y: (x==y).all(), state.opt_state, new_state.opt_state)\n\n(ScaleByAdamState(count=Array(True, dtype=bool), mu=FrozenDict({\n     Conv_0: {\n         bias: Array(True, dtype=bool),\n         kernel: Array(True, dtype=bool),\n     },\n     Conv_1: {\n         bias: Array(True, dtype=bool),\n         kernel: Array(True, dtype=bool),\n     },\n     Dense_0: {\n         bias: Array(True, dtype=bool),\n         kernel: Array(True, dtype=bool),\n     },\n }), nu=FrozenDict({\n     Conv_0: {\n         bias: Array(True, dtype=bool),\n         kernel: Array(True, dtype=bool),\n     },\n     Conv_1: {\n         bias: Array(True, dtype=bool),\n         kernel: Array(True, dtype=bool),\n     },\n     Dense_0: {\n         bias: Array(True, dtype=bool),\n         kernel: Array(True, dtype=bool),\n     },\n })),\n EmptyState())"
  },
  {
    "objectID": "03_Utils/01_wandb_weights_logging.html",
    "href": "03_Utils/01_wandb_weights_logging.html",
    "title": "W&B Weights Logging!",
    "section": "",
    "text": "import tensorflow as tf\ntf.config.set_visible_devices([], device_type='GPU')\n\n2023-05-06 17:48:45.358740: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2023-05-06 17:48:45.409308: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-05-06 17:48:47.394403: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nfrom typing import Any, Callable, Sequence, Union\nimport numpy as np\n\nimport jax\nfrom jax import lax, random, numpy as jnp\nimport flax\nfrom flax.core import freeze, unfreeze, FrozenDict\nfrom flax import linen as nn\nfrom flax import struct\nfrom flax.training import train_state\n\nimport optax\n\nfrom clu import metrics\nfrom ml_collections import ConfigDict\n\nfrom einops import reduce\nimport wandb\n\nfrom iqadatasets.datasets import *\nfrom fxlayers.layers import *"
  },
  {
    "objectID": "03_Utils/01_wandb_weights_logging.html#load-the-data",
    "href": "03_Utils/01_wandb_weights_logging.html#load-the-data",
    "title": "W&B Weights Logging!",
    "section": "Load the data",
    "text": "Load the data\n\nWe’re going to employ iqadatasets to ease the loading of the data.\n\n\ndst_train = TID2008(\"/media/disk/databases/BBDD_video_image/Image_Quality/TID/TID2008/\")\ndst_val = TID2013(\"/media/disk/databases/BBDD_video_image/Image_Quality/TID/TID2013/\")\n\n\nimg, img_dist, mos = next(iter(dst_train.dataset))\nimg.shape, img_dist.shape, mos.shape\n\n2023-05-06 17:48:54.028961: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n(TensorShape([384, 512, 3]), TensorShape([384, 512, 3]), TensorShape([]))\n\n\n\nimg, img_dist, mos = next(iter(dst_val.dataset))\nimg.shape, img_dist.shape, mos.shape\n\n2023-05-06 17:48:54.412279: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n(TensorShape([384, 512, 3]), TensorShape([384, 512, 3]), TensorShape([]))\n\n\n\nconfig = {\n    \"BATCH_SIZE\": 32,\n    \"EPOCHS\": 5,\n    \"LEARNING_RATE\": 3e-4,\n}\nconfig = ConfigDict(config)\nconfig\n\nBATCH_SIZE: 32\nEPOCHS: 5\nLEARNING_RATE: 0.0003\n\n\n\nwandb.init(project=\"Testing_JAX\",\n           job_type=\"training\",\n           config=config,\n           mode=\"online\",\n           )\nconfig = config\nconfig\n\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\nwandb: Currently logged in as: jorgvt. Use `wandb login --relogin` to force relogin\n\n\nwandb version 0.15.2 is available!  To upgrade, please run:\n $ pip install wandb --upgrade\n\n\nTracking run with wandb version 0.15.0\n\n\nRun data is saved locally in /media/disk/users/vitojor/JaxPlayground/Notebooks/03_Utils/wandb/run-20230506_174857-uwiucf4i\n\n\nSyncing run dandy-spaceship-1 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/jorgvt/Testing_JAX\n\n\n View run at https://wandb.ai/jorgvt/Testing_JAX/runs/uwiucf4i\n\n\nBATCH_SIZE: 32\nEPOCHS: 5\nLEARNING_RATE: 0.0003\n\n\n\ndst_train_rdy = dst_train.dataset.batch(config.BATCH_SIZE, drop_remainder=True)\ndst_val_rdy = dst_val.dataset.batch(config.BATCH_SIZE, drop_remainder=True)"
  },
  {
    "objectID": "03_Utils/01_wandb_weights_logging.html#define-the-model-were-going-to-use",
    "href": "03_Utils/01_wandb_weights_logging.html#define-the-model-were-going-to-use",
    "title": "W&B Weights Logging!",
    "section": "Define the model we’re going to use",
    "text": "Define the model we’re going to use\n\nIt’s going to be a very simple model just for demonstration purposes.\n\n\nclass GDN(nn.Module):\n    \"\"\"Generalized Divisive Normalization.\"\"\"\n    kernel_size: Union[int, Sequence[int]]\n    strides: int = 1\n    padding: str = \"SAME\"\n    apply_independently: bool = False\n    # feature_group_count: int = 1\n    kernel_init: Callable = nn.initializers.lecun_normal()\n    bias_init: Callable = nn.initializers.ones_init()\n    alpha: float = 2.\n    epsilon: float = 1/2 # Exponential of the denominator\n    eps: float = 1e-6 # Numerical stability in the denominator\n\n    @nn.compact\n    def __call__(self,\n                 inputs,\n                 ):\n        denom = nn.Conv(features=inputs.shape[-1], # Same output channels as input\n                        kernel_size=self.kernel_size if isinstance(self.kernel_size, Sequence) else [self.kernel_size]*2, \n                        strides=self.strides, \n                        padding=self.padding,\n                        # feature_group_count=self.feature_group_count,\n                        feature_group_count=inputs.shape[-1] if self.apply_independently else 1,\n                        kernel_init=self.kernel_init, \n                        bias_init=self.bias_init)(inputs**self.alpha)\n        denom = nn.relu(denom)\n        return inputs / (denom**self.epsilon + self.eps)\n\n\nclass PerceptNet(nn.Module):\n    \"\"\"IQA model inspired by the visual system.\"\"\"\n\n    @nn.compact\n    def __call__(self,\n                 inputs,\n                 **kwargs,\n                 ):\n        outputs = nn.Conv(features=3, kernel_size=(5,5), strides=1, padding=\"SAME\")(inputs)\n        outputs = GDN(kernel_size=1, strides=1, padding=\"SAME\", apply_independently=True)(outputs)\n        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n        outputs = nn.Conv(features=6, kernel_size=(5,5), strides=1, padding=\"SAME\")(outputs)\n        outputs = GDN(kernel_size=1, strides=1, padding=\"SAME\", apply_independently=False)(outputs)\n        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n        # outputs = GaborLayer(features=128, kernel_size=21, strides=1, padding=\"SAME\", fs=21)(outputs, **kwargs)\n        outputs = nn.Conv(features=128, kernel_size=(5,5), strides=1, padding=\"SAME\")(outputs)\n        outputs = GDN(kernel_size=1, strides=1, padding=\"SAME\", apply_independently=False)(outputs)\n        return outputs"
  },
  {
    "objectID": "03_Utils/01_wandb_weights_logging.html#define-the-metrics-with-clu",
    "href": "03_Utils/01_wandb_weights_logging.html#define-the-metrics-with-clu",
    "title": "W&B Weights Logging!",
    "section": "Define the metrics with clu",
    "text": "Define the metrics with clu\n\n@struct.dataclass\nclass Metrics(metrics.Collection):\n    \"\"\"Collection of metrics to be tracked during training.\"\"\"\n    loss: metrics.Average.from_output(\"loss\")\n\nBy default, TrainState doesn’t include metrics, but it’s very easy to subclass it so that it does:\n\nclass TrainState(train_state.TrainState):\n    metrics: Metrics\n    state: FrozenDict\n\nWe’ll define a function that initializes the TrainState from a module, a rng key and some optimizer:\n\ndef create_train_state(module, key, tx, input_shape):\n    \"\"\"Creates the initial `TrainState`.\"\"\"\n    variables = module.init(key, jnp.ones(input_shape))\n    state, params = variables.pop('params')\n    return TrainState.create(\n        apply_fn=module.apply,\n        params=params,\n        state=state,\n        tx=tx,\n        metrics=Metrics.empty()\n    )"
  },
  {
    "objectID": "03_Utils/01_wandb_weights_logging.html#defining-the-training-step",
    "href": "03_Utils/01_wandb_weights_logging.html#defining-the-training-step",
    "title": "W&B Weights Logging!",
    "section": "Defining the training step",
    "text": "Defining the training step\n\nWe want to write a function that takes the TrainState and a batch of data can performs an optimization step.\n\n\ndef pearson_correlation(vec1, vec2):\n    vec1 = vec1.squeeze()\n    vec2 = vec2.squeeze()\n    vec1_mean = vec1.mean()\n    vec2_mean = vec2.mean()\n    num = vec1-vec1_mean\n    num *= vec2-vec2_mean\n    num = num.sum()\n    denom = jnp.sqrt(jnp.sum((vec1-vec1_mean)**2))\n    denom *= jnp.sqrt(jnp.sum((vec2-vec2_mean)**2))\n    return num/denom\n\n\n@jax.jit\ndef train_step(state, batch):\n    \"\"\"Train for a single step.\"\"\"\n    img, img_dist, mos = batch\n    def loss_fn(params):\n        ## Forward pass through the model\n        img_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img, mutable=list(state.state.keys()), train=True)\n        img_dist_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img_dist, mutable=list(state.state.keys()), train=True)\n\n        ## Calculate the distance\n        dist = ((img_pred - img_dist_pred)**2).sum(axis=(1,2,3))**(1/2)\n        \n        ## Calculate pearson correlation\n        return pearson_correlation(dist, mos), updated_state\n    \n    (loss, updated_state), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n    state = state.apply_gradients(grads=grads)\n    metrics_updates = state.metrics.single_from_model_output(loss=loss)\n    metrics = state.metrics.merge(metrics_updates)\n    state = state.replace(metrics=metrics)\n    state = state.replace(state=updated_state)\n    return state\n\nIn their example, they don’t calculate the metrics at the same time. I think it is kind of a waste because it means having to perform a new forward pass, but we’ll follow as of now. Let’s define a function to perform metric calculation:\n\n@jax.jit\ndef compute_metrics(*, state, batch):\n    \"\"\"Obtaining the metrics for a given batch.\"\"\"\n    img, img_dist, mos = batch\n    def loss_fn(params):\n        ## Forward pass through the model\n        img_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img, mutable=list(state.state.keys()), train=True)\n        img_dist_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img_dist, mutable=list(state.state.keys()), train=True)\n\n        ## Calculate the distance\n        dist = ((img_pred - img_dist_pred)**2).sum(axis=(1,2,3))**(1/2)\n        \n        ## Calculate pearson correlation\n        return pearson_correlation(dist, mos)\n    \n    metrics_updates = state.metrics.single_from_model_output(loss=loss_fn(state.params))\n    metrics = state.metrics.merge(metrics_updates)\n    state = state.replace(metrics=metrics)\n    return state"
  },
  {
    "objectID": "03_Utils/01_wandb_weights_logging.html#train-the-model",
    "href": "03_Utils/01_wandb_weights_logging.html#train-the-model",
    "title": "W&B Weights Logging!",
    "section": "Train the model!",
    "text": "Train the model!\n\nstate = create_train_state(PerceptNet(), random.PRNGKey(0), optax.adam(config.LEARNING_RATE), input_shape=(1,384,512,3))\n\n\nmetrics_history = {\n    \"train_loss\": [],\n    \"val_loss\": [],\n}\n\n\ndef flatten_params(params): return {\"/\".join(k): v for k, v in flax.traverse_util.flatten_dict(params).items()}\n\n\nfor epoch in range(config.EPOCHS):\n    ## Training\n    for batch in dst_train_rdy.as_numpy_iterator():\n        state = train_step(state, batch)\n        # state = compute_metrics(state=state, batch=batch)\n        break\n\n    ## Log the metrics\n    for name, value in state.metrics.compute().items():\n        metrics_history[f\"train_{name}\"].append(value)\n    \n    ## Empty the metrics\n    state = state.replace(metrics=state.metrics.empty())\n\n    ## Evaluation\n    for batch in dst_val_rdy.as_numpy_iterator():\n        state = compute_metrics(state=state, batch=batch)\n        break\n    for name, value in state.metrics.compute().items():\n        metrics_history[f\"val_{name}\"].append(value)\n    state = state.replace(metrics=state.metrics.empty())\n    \n    wandb.log({f\"{k}\": wandb.Histogram(v) for k, v in flatten_params(state.params).items()}, commit=False)\n    wandb.log({\"epoch\": epoch+1, **{name:values[-1] for name, values in metrics_history.items()}})\n    print(f'Epoch {epoch} -&gt; [Train] Loss: {metrics_history[\"train_loss\"][-1]} [Val] Loss: {metrics_history[\"val_loss\"][-1]}')\n    # break\n\n2023-05-06 17:49:16.962630: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-05-06 17:49:30.612070: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\nEpoch 0 -&gt; [Train] Loss: -0.20941223204135895 [Val] Loss: -0.4911959171295166\nEpoch 1 -&gt; [Train] Loss: -0.2313786745071411 [Val] Loss: -0.49721601605415344\nEpoch 2 -&gt; [Train] Loss: -0.28192612528800964 [Val] Loss: -0.5022366046905518\nEpoch 3 -&gt; [Train] Loss: -0.4186874032020569 [Val] Loss: -0.510519802570343\nEpoch 4 -&gt; [Train] Loss: -0.5153611898422241 [Val] Loss: -0.5393363237380981\nCPU times: user 33.6 s, sys: 22.4 s, total: 56 s\nWall time: 19.7 s\n\n\n\nwandb.finish()\n\nWaiting for W&B process to finish... (success).\n\n\n\n\nRun history:\n\n\n\nepoch\n▁▃▅▆█\n\n\ntrain_loss\n█▇▆▃▁\n\n\nval_loss\n█▇▆▅▁\n\n\n\n\n\n\nRun summary:\n\n\n\n\n\nepoch\n5\n\n\ntrain_loss\n-0.51536\n\n\nval_loss\n-0.53934\n\n\n\n\n\n\n\n View run dandy-spaceship-1 at: https://wandb.ai/jorgvt/Testing_JAX/runs/uwiucf4iSynced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20230506_174857-uwiucf4i/logs"
  },
  {
    "objectID": "03_Utils/00_parameter_constrains.html",
    "href": "03_Utils/00_parameter_constrains.html",
    "title": "Parameter constraints",
    "section": "",
    "text": "The idea is basically traversing the weights PyTree and applying a function over the weights, which souldn’t be specifically hard given we have jax.tree_util.tree_map. Then we can add that to a usual training loop to ensure that we can keep reaplying the same constraint during training to, for example, keep a specific set of weights above 0 at all times.\nimport tensorflow as tf\ntf.config.set_visible_devices([], device_type='GPU')\n\n2023-05-06 16:42:08.061274: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2023-05-06 16:42:08.111834: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-05-06 16:42:10.456447: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nfrom typing import Any, Callable, Sequence, Union\nimport numpy as np\n\nimport jax\nfrom jax import lax, random, numpy as jnp\nimport flax\nfrom flax.core import freeze, unfreeze, FrozenDict\nfrom flax import linen as nn\nfrom flax import struct\nfrom flax.training import train_state\n\nimport optax\n\nfrom clu import metrics\nfrom ml_collections import ConfigDict\n\nfrom einops import reduce\n\nfrom iqadatasets.datasets import *\nfrom fxlayers.layers import *"
  },
  {
    "objectID": "03_Utils/00_parameter_constrains.html#load-the-data",
    "href": "03_Utils/00_parameter_constrains.html#load-the-data",
    "title": "Parameter constraints",
    "section": "Load the data",
    "text": "Load the data\n\nWe’re going to employ iqadatasets to ease the loading of the data.\n\n\ndst_train = TID2008(\"/media/disk/databases/BBDD_video_image/Image_Quality/TID/TID2008/\")\ndst_val = TID2013(\"/media/disk/databases/BBDD_video_image/Image_Quality/TID/TID2013/\")\n\n\nimg, img_dist, mos = next(iter(dst_train.dataset))\nimg.shape, img_dist.shape, mos.shape\n\n2023-05-06 16:42:16.513531: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n(TensorShape([384, 512, 3]), TensorShape([384, 512, 3]), TensorShape([]))\n\n\n\nimg, img_dist, mos = next(iter(dst_val.dataset))\nimg.shape, img_dist.shape, mos.shape\n\n2023-05-06 16:42:16.806541: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n(TensorShape([384, 512, 3]), TensorShape([384, 512, 3]), TensorShape([]))\n\n\n\nconfig = {\n    \"BATCH_SIZE\": 32,\n    \"EPOCHS\": 5,\n    \"LEARNING_RATE\": 3e-4,\n}\nconfig = ConfigDict(config)\nconfig\n\nBATCH_SIZE: 32\nEPOCHS: 5\nLEARNING_RATE: 0.0003\n\n\n\ndst_train_rdy = dst_train.dataset.batch(config.BATCH_SIZE, drop_remainder=True)\ndst_val_rdy = dst_val.dataset.batch(config.BATCH_SIZE, drop_remainder=True)"
  },
  {
    "objectID": "03_Utils/00_parameter_constrains.html#define-the-model-were-going-to-use",
    "href": "03_Utils/00_parameter_constrains.html#define-the-model-were-going-to-use",
    "title": "Parameter constraints",
    "section": "Define the model we’re going to use",
    "text": "Define the model we’re going to use\n\nIt’s going to be a very simple model just for demonstration purposes.\n\n\nclass GDN(nn.Module):\n    \"\"\"Generalized Divisive Normalization.\"\"\"\n    kernel_size: Union[int, Sequence[int]]\n    strides: int = 1\n    padding: str = \"SAME\"\n    apply_independently: bool = False\n    # feature_group_count: int = 1\n    kernel_init: Callable = nn.initializers.lecun_normal()\n    bias_init: Callable = nn.initializers.ones_init()\n    alpha: float = 2.\n    epsilon: float = 1/2 # Exponential of the denominator\n    eps: float = 1e-6 # Numerical stability in the denominator\n\n    @nn.compact\n    def __call__(self,\n                 inputs,\n                 ):\n        denom = nn.Conv(features=inputs.shape[-1], # Same output channels as input\n                        kernel_size=self.kernel_size if isinstance(self.kernel_size, Sequence) else [self.kernel_size]*2, \n                        strides=self.strides, \n                        padding=self.padding,\n                        # feature_group_count=self.feature_group_count,\n                        feature_group_count=inputs.shape[-1] if self.apply_independently else 1,\n                        kernel_init=self.kernel_init, \n                        bias_init=nn.initializers.constant(-1))(inputs**self.alpha)\n        # denom = nn.relu(denom)\n        return inputs / (denom**self.epsilon + self.eps)\n\n\nclass PerceptNet(nn.Module):\n    \"\"\"IQA model inspired by the visual system.\"\"\"\n\n    @nn.compact\n    def __call__(self,\n                 inputs,\n                 **kwargs,\n                 ):\n        outputs = nn.Conv(features=3, kernel_size=(5,5), strides=1, padding=\"SAME\")(inputs)\n        outputs = GDN(kernel_size=1, strides=1, padding=\"SAME\", apply_independently=True)(outputs)\n        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n        outputs = nn.Conv(features=6, kernel_size=(5,5), strides=1, padding=\"SAME\")(outputs)\n        outputs = GDN(kernel_size=1, strides=1, padding=\"SAME\", apply_independently=False)(outputs)\n        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n        # outputs = GaborLayer(features=128, kernel_size=21, strides=1, padding=\"SAME\", fs=21)(outputs, **kwargs)\n        outputs = nn.Conv(features=128, kernel_size=(5,5), strides=1, padding=\"SAME\")(outputs)\n        outputs = GDN(kernel_size=1, strides=1, padding=\"SAME\", apply_independently=False)(outputs)\n        return outputs"
  },
  {
    "objectID": "03_Utils/00_parameter_constrains.html#define-the-metrics-with-clu",
    "href": "03_Utils/00_parameter_constrains.html#define-the-metrics-with-clu",
    "title": "Parameter constraints",
    "section": "Define the metrics with clu",
    "text": "Define the metrics with clu\n\n@struct.dataclass\nclass Metrics(metrics.Collection):\n    \"\"\"Collection of metrics to be tracked during training.\"\"\"\n    loss: metrics.Average.from_output(\"loss\")\n\nBy default, TrainState doesn’t include metrics, but it’s very easy to subclass it so that it does:\n\nclass TrainState(train_state.TrainState):\n    metrics: Metrics\n    state: FrozenDict\n\nWe’ll define a function that initializes the TrainState from a module, a rng key and some optimizer:\n\ndef create_train_state(module, key, tx, input_shape):\n    \"\"\"Creates the initial `TrainState`.\"\"\"\n    variables = module.init(key, jnp.ones(input_shape))\n    state, params = variables.pop('params')\n    return TrainState.create(\n        apply_fn=module.apply,\n        params=params,\n        state=state,\n        tx=tx,\n        metrics=Metrics.empty()\n    )"
  },
  {
    "objectID": "03_Utils/00_parameter_constrains.html#defining-the-training-step",
    "href": "03_Utils/00_parameter_constrains.html#defining-the-training-step",
    "title": "Parameter constraints",
    "section": "Defining the training step",
    "text": "Defining the training step\n\nWe want to write a function that takes the TrainState and a batch of data can performs an optimization step.\n\n\ndef pearson_correlation(vec1, vec2):\n    vec1 = vec1.squeeze()\n    vec2 = vec2.squeeze()\n    vec1_mean = vec1.mean()\n    vec2_mean = vec2.mean()\n    num = vec1-vec1_mean\n    num *= vec2-vec2_mean\n    num = num.sum()\n    denom = jnp.sqrt(jnp.sum((vec1-vec1_mean)**2))\n    denom *= jnp.sqrt(jnp.sum((vec2-vec2_mean)**2))\n    return num/denom\n\n\n@jax.jit\ndef train_step(state, batch):\n    \"\"\"Train for a single step.\"\"\"\n    img, img_dist, mos = batch\n    def loss_fn(params):\n        ## Forward pass through the model\n        img_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img, mutable=list(state.state.keys()), train=True)\n        img_dist_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img_dist, mutable=list(state.state.keys()), train=True)\n\n        ## Calculate the distance\n        dist = ((img_pred - img_dist_pred)**2).sum(axis=(1,2,3))**(1/2)\n        \n        ## Calculate pearson correlation\n        return pearson_correlation(dist, mos), updated_state\n    \n    (loss, updated_state), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n    state = state.apply_gradients(grads=grads)\n    metrics_updates = state.metrics.single_from_model_output(loss=loss)\n    metrics = state.metrics.merge(metrics_updates)\n    state = state.replace(metrics=metrics)\n    state = state.replace(state=updated_state)\n    return state\n\nIn their example, they don’t calculate the metrics at the same time. I think it is kind of a waste because it means having to perform a new forward pass, but we’ll follow as of now. Let’s define a function to perform metric calculation:\n\n@jax.jit\ndef compute_metrics(*, state, batch):\n    \"\"\"Obtaining the metrics for a given batch.\"\"\"\n    img, img_dist, mos = batch\n    def loss_fn(params):\n        ## Forward pass through the model\n        img_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img, mutable=list(state.state.keys()), train=True)\n        img_dist_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img_dist, mutable=list(state.state.keys()), train=True)\n\n        ## Calculate the distance\n        dist = ((img_pred - img_dist_pred)**2).sum(axis=(1,2,3))**(1/2)\n        \n        ## Calculate pearson correlation\n        return pearson_correlation(dist, mos)\n    \n    metrics_updates = state.metrics.single_from_model_output(loss=loss_fn(state.params))\n    metrics = state.metrics.merge(metrics_updates)\n    state = state.replace(metrics=metrics)\n    return state"
  },
  {
    "objectID": "03_Utils/00_parameter_constrains.html#train-the-model",
    "href": "03_Utils/00_parameter_constrains.html#train-the-model",
    "title": "Parameter constraints",
    "section": "Train the model!",
    "text": "Train the model!\nFirst we’ll instantiate the TrainState to get our parameters PyTree:\n\nstate = create_train_state(PerceptNet(), random.PRNGKey(0), optax.adam(config.LEARNING_RATE), input_shape=(1,384,512,3))\n\nAnd now we can apply flax.traverse_util.path_aware_map() together with jnp.clip() to ensure that there are no negative numbers in our GDN parameters:\n\nclipped_params = flax.traverse_util.path_aware_map(lambda path, v: jnp.clip(v, 0) if \"GDN\" in \"_\".join(path) else v, state.params)\nclipped_params.keys()\n\ndict_keys(['Conv_0', 'GDN_0', 'Conv_1', 'GDN_1', 'Conv_2', 'GDN_2'])\n\n\n\nstate.params[\"GDN_1\"][\"Conv_0\"], clipped_params[\"GDN_1\"][\"Conv_0\"]\n\n(FrozenDict({\n     kernel: Array([[[[-0.20927475, -0.8931865 , -0.01957742,  0.39708266,\n                0.57847786,  0.4533468 ],\n              [ 0.06100697, -0.28133187,  0.50638074, -0.0699495 ,\n               -0.6131419 , -0.25098443],\n              [-0.38723496, -0.37814805,  0.07733525,  0.68892664,\n               -0.2935091 ,  0.3425824 ],\n              [-0.15717277, -0.65036553,  0.5565868 , -0.20142964,\n               -0.10007945,  0.19361018],\n              [-0.29459316, -0.65032095, -0.30642185, -0.52035373,\n                0.00798638,  0.73876256],\n              [-0.11728425, -0.7334936 , -0.70490634, -0.28179142,\n               -0.2679925 ,  0.08794045]]]], dtype=float32),\n     bias: Array([-1., -1., -1., -1., -1., -1.], dtype=float32),\n }),\n {'kernel': Array([[[[0.        , 0.        , 0.        , 0.39708266, 0.57847786,\n            0.4533468 ],\n           [0.06100697, 0.        , 0.50638074, 0.        , 0.        ,\n            0.        ],\n           [0.        , 0.        , 0.07733525, 0.68892664, 0.        ,\n            0.3425824 ],\n           [0.        , 0.        , 0.5565868 , 0.        , 0.        ,\n            0.19361018],\n           [0.        , 0.        , 0.        , 0.        , 0.00798638,\n            0.73876256],\n           [0.        , 0.        , 0.        , 0.        , 0.        ,\n            0.08794045]]]], dtype=float32),\n  'bias': Array([0., 0., 0., 0., 0., 0.], dtype=float32)})\n\n\nActually, our procedure will clip values both in kernel and in bias. If we want to restrict ourselves to only the kernel, we can do the following:\n\nclipped_params_kernel = flax.traverse_util.path_aware_map(lambda path, v: jnp.clip(v, 0) if (\"GDN\" in \"_\".join(path)) and (\"kernel\" in path) else v, state.params)\nclipped_params_kernel.keys()\n\ndict_keys(['Conv_0', 'GDN_0', 'Conv_1', 'GDN_1', 'Conv_2', 'GDN_2'])\n\n\n\nstate.params[\"GDN_1\"][\"Conv_0\"], clipped_params_kernel[\"GDN_1\"][\"Conv_0\"]\n\n(FrozenDict({\n     kernel: Array([[[[-0.20927475, -0.8931865 , -0.01957742,  0.39708266,\n                0.57847786,  0.4533468 ],\n              [ 0.06100697, -0.28133187,  0.50638074, -0.0699495 ,\n               -0.6131419 , -0.25098443],\n              [-0.38723496, -0.37814805,  0.07733525,  0.68892664,\n               -0.2935091 ,  0.3425824 ],\n              [-0.15717277, -0.65036553,  0.5565868 , -0.20142964,\n               -0.10007945,  0.19361018],\n              [-0.29459316, -0.65032095, -0.30642185, -0.52035373,\n                0.00798638,  0.73876256],\n              [-0.11728425, -0.7334936 , -0.70490634, -0.28179142,\n               -0.2679925 ,  0.08794045]]]], dtype=float32),\n     bias: Array([-1., -1., -1., -1., -1., -1.], dtype=float32),\n }),\n {'kernel': Array([[[[0.        , 0.        , 0.        , 0.39708266, 0.57847786,\n            0.4533468 ],\n           [0.06100697, 0.        , 0.50638074, 0.        , 0.        ,\n            0.        ],\n           [0.        , 0.        , 0.07733525, 0.68892664, 0.        ,\n            0.3425824 ],\n           [0.        , 0.        , 0.5565868 , 0.        , 0.        ,\n            0.19361018],\n           [0.        , 0.        , 0.        , 0.        , 0.00798638,\n            0.73876256],\n           [0.        , 0.        , 0.        , 0.        , 0.        ,\n            0.08794045]]]], dtype=float32),\n  'bias': Array([-1., -1., -1., -1., -1., -1.], dtype=float32)})\n\n\nIf we want to apply this at each step of the training process, we can wrap everything inside a function and add a “constraint step” into our training loop:\n\ndef clip_layer_kernel(params, # PyTree containing the parameters to clip.\n                      layer_name: str, # String indicating the name of the layer. It can be a generic like \"Conv\" or specific like \"Conv_0\". \n                      a_min=None, # Min value to clip to.\n                      a_max=None, # Max value to clip to.\n                      ): # Same PyTree as `params` but with the corresponding values clipped.\n    return freeze(flax.traverse_util.path_aware_map(lambda path, v: jnp.clip(v, a_min, a_max) if (layer_name in \"_\".join(path)) and (\"kernel\" in path) else v, params))\n\n\ndef clip_layer(params, # PyTree containing the parameters to clip.\n               layer_name: str, # String indicating the name of the layer. It can be a generic like \"Conv\" or specific like \"Conv_0\". \n               a_min=None, # Min value to clip to.\n               a_max=None, # Max value to clip to.\n               ): # Same PyTree as `params` but with the corresponding values clipped.\n    return freeze(flax.traverse_util.path_aware_map(lambda path, v: jnp.clip(v, a_min, a_max) if layer_name in \"_\".join(path) else v, params))\n\n\n@jax.jit\ndef train_step(state, batch):\n    \"\"\"Train for a single step.\"\"\"\n    img, img_dist, mos = batch\n    def loss_fn(params):\n        ## Forward pass through the model\n        img_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img, mutable=list(state.state.keys()), train=True)\n        img_dist_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img_dist, mutable=list(state.state.keys()), train=True)\n\n        ## Calculate the distance\n        dist = ((img_pred - img_dist_pred)**2).sum(axis=(1,2,3))**(1/2)\n        \n        ## Calculate pearson correlation\n        return pearson_correlation(dist, mos), updated_state\n    \n    (loss, updated_state), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n    state = state.apply_gradients(grads=grads)\n    metrics_updates = state.metrics.single_from_model_output(loss=loss)\n    metrics = state.metrics.merge(metrics_updates)\n    state = state.replace(metrics=metrics)\n    state = state.replace(state=updated_state)\n    return state"
  },
  {
    "objectID": "03_Utils/00_parameter_constrains.html#train-the-model-1",
    "href": "03_Utils/00_parameter_constrains.html#train-the-model-1",
    "title": "Parameter constraints",
    "section": "Train the model!",
    "text": "Train the model!\n\nclass GDN(nn.Module):\n    \"\"\"Generalized Divisive Normalization.\"\"\"\n    kernel_size: Union[int, Sequence[int]]\n    strides: int = 1\n    padding: str = \"SAME\"\n    apply_independently: bool = False\n    # feature_group_count: int = 1\n    kernel_init: Callable = nn.initializers.lecun_normal()\n    bias_init: Callable = nn.initializers.ones_init()\n    alpha: float = 2.\n    epsilon: float = 1/2 # Exponential of the denominator\n    eps: float = 1e-6 # Numerical stability in the denominator\n\n    @nn.compact\n    def __call__(self,\n                 inputs,\n                 ):\n        denom = nn.Conv(features=inputs.shape[-1], # Same output channels as input\n                        kernel_size=self.kernel_size if isinstance(self.kernel_size, Sequence) else [self.kernel_size]*2, \n                        strides=self.strides, \n                        padding=self.padding,\n                        # feature_group_count=self.feature_group_count,\n                        feature_group_count=inputs.shape[-1] if self.apply_independently else 1,\n                        kernel_init=self.kernel_init, \n                        bias_init=nn.initializers.ones)(inputs**self.alpha)\n        # denom = nn.relu(denom)\n        return inputs / (denom**self.epsilon + self.eps)\n\n\nstate = create_train_state(PerceptNet(), random.PRNGKey(0), optax.adam(config.LEARNING_RATE), input_shape=(1,384,512,3))\nstate = state.replace(params=clip_layer(state.params, \"GDN\", a_min=0))\n\n\nmetrics_history = {\n    \"train_loss\": [],\n    \"val_loss\": [],\n}\n\n\nfor epoch in range(config.EPOCHS):\n    ## Training\n    for batch in dst_train_rdy.as_numpy_iterator():\n        state = train_step(state, batch)\n        state = state.replace(params=clip_layer(state.params, \"GDN\", a_min=0))\n        # state = compute_metrics(state=state, batch=batch)\n        break\n\n    ## Log the metrics\n    for name, value in state.metrics.compute().items():\n        metrics_history[f\"train_{name}\"].append(value)\n    \n    ## Empty the metrics\n    state = state.replace(metrics=state.metrics.empty())\n\n    ## Evaluation\n    for batch in dst_val_rdy.as_numpy_iterator():\n        state = compute_metrics(state=state, batch=batch)\n        break\n    for name, value in state.metrics.compute().items():\n        metrics_history[f\"val_{name}\"].append(value)\n    state = state.replace(metrics=state.metrics.empty())\n    \n    print(f'Epoch {epoch} -&gt; [Train] Loss: {metrics_history[\"train_loss\"][-1]} [Val] Loss: {metrics_history[\"val_loss\"][-1]}')\n    break\n\nEpoch 0 -&gt; [Train] Loss: -0.6268874406814575 [Val] Loss: -0.6873180866241455\nCPU times: user 14.6 s, sys: 6.73 s, total: 21.3 s\nWall time: 3.82 s"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "JaxPlayground",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  }
]